{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 15610,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.032030749519538756,
      "grad_norm": 7.919735431671143,
      "learning_rate": 1.4099999999999999e-05,
      "loss": 2.6859,
      "step": 50
    },
    {
      "epoch": 0.06406149903907751,
      "grad_norm": 8.488167762756348,
      "learning_rate": 2.91e-05,
      "loss": 1.9585,
      "step": 100
    },
    {
      "epoch": 0.09609224855861627,
      "grad_norm": 7.225945949554443,
      "learning_rate": 2.9909090909090908e-05,
      "loss": 1.896,
      "step": 150
    },
    {
      "epoch": 0.12812299807815503,
      "grad_norm": 6.0624542236328125,
      "learning_rate": 2.981237911025145e-05,
      "loss": 1.7062,
      "step": 200
    },
    {
      "epoch": 0.1601537475976938,
      "grad_norm": 8.860013008117676,
      "learning_rate": 2.9717601547388782e-05,
      "loss": 1.8406,
      "step": 250
    },
    {
      "epoch": 0.19218449711723254,
      "grad_norm": 7.789841651916504,
      "learning_rate": 2.9620889748549324e-05,
      "loss": 1.6827,
      "step": 300
    },
    {
      "epoch": 0.2242152466367713,
      "grad_norm": 6.809055805206299,
      "learning_rate": 2.9524177949709862e-05,
      "loss": 1.662,
      "step": 350
    },
    {
      "epoch": 0.25624599615631005,
      "grad_norm": 5.776876449584961,
      "learning_rate": 2.9427466150870408e-05,
      "loss": 1.6517,
      "step": 400
    },
    {
      "epoch": 0.2882767456758488,
      "grad_norm": 5.424187660217285,
      "learning_rate": 2.933075435203095e-05,
      "loss": 1.6218,
      "step": 450
    },
    {
      "epoch": 0.3203074951953876,
      "grad_norm": 5.329038143157959,
      "learning_rate": 2.923404255319149e-05,
      "loss": 1.643,
      "step": 500
    },
    {
      "epoch": 0.3523382447149263,
      "grad_norm": 7.3127121925354,
      "learning_rate": 2.9137330754352034e-05,
      "loss": 1.5667,
      "step": 550
    },
    {
      "epoch": 0.3843689942344651,
      "grad_norm": 5.755527973175049,
      "learning_rate": 2.9040618955512572e-05,
      "loss": 1.5385,
      "step": 600
    },
    {
      "epoch": 0.41639974375400385,
      "grad_norm": 5.285611629486084,
      "learning_rate": 2.8943907156673114e-05,
      "loss": 1.5234,
      "step": 650
    },
    {
      "epoch": 0.4484304932735426,
      "grad_norm": 6.730897426605225,
      "learning_rate": 2.8847195357833656e-05,
      "loss": 1.584,
      "step": 700
    },
    {
      "epoch": 0.48046124279308133,
      "grad_norm": 7.115546703338623,
      "learning_rate": 2.8750483558994198e-05,
      "loss": 1.5882,
      "step": 750
    },
    {
      "epoch": 0.5124919923126201,
      "grad_norm": 5.131287097930908,
      "learning_rate": 2.865377176015474e-05,
      "loss": 1.5053,
      "step": 800
    },
    {
      "epoch": 0.5445227418321589,
      "grad_norm": 6.124546051025391,
      "learning_rate": 2.8557059961315282e-05,
      "loss": 1.5778,
      "step": 850
    },
    {
      "epoch": 0.5765534913516976,
      "grad_norm": 6.889605522155762,
      "learning_rate": 2.8460348162475824e-05,
      "loss": 1.4663,
      "step": 900
    },
    {
      "epoch": 0.6085842408712364,
      "grad_norm": 5.710167407989502,
      "learning_rate": 2.8363636363636363e-05,
      "loss": 1.5037,
      "step": 950
    },
    {
      "epoch": 0.6406149903907752,
      "grad_norm": 8.389203071594238,
      "learning_rate": 2.8266924564796905e-05,
      "loss": 1.4089,
      "step": 1000
    },
    {
      "epoch": 0.672645739910314,
      "grad_norm": 5.347212791442871,
      "learning_rate": 2.8170212765957447e-05,
      "loss": 1.4311,
      "step": 1050
    },
    {
      "epoch": 0.7046764894298526,
      "grad_norm": 5.771870136260986,
      "learning_rate": 2.807350096711799e-05,
      "loss": 1.4931,
      "step": 1100
    },
    {
      "epoch": 0.7367072389493914,
      "grad_norm": 4.423165321350098,
      "learning_rate": 2.797678916827853e-05,
      "loss": 1.5548,
      "step": 1150
    },
    {
      "epoch": 0.7687379884689302,
      "grad_norm": 5.574798107147217,
      "learning_rate": 2.7880077369439073e-05,
      "loss": 1.4333,
      "step": 1200
    },
    {
      "epoch": 0.8007687379884689,
      "grad_norm": 4.586747169494629,
      "learning_rate": 2.7783365570599615e-05,
      "loss": 1.5117,
      "step": 1250
    },
    {
      "epoch": 0.8327994875080077,
      "grad_norm": 6.976444721221924,
      "learning_rate": 2.7688588007736943e-05,
      "loss": 1.396,
      "step": 1300
    },
    {
      "epoch": 0.8648302370275465,
      "grad_norm": 5.5901947021484375,
      "learning_rate": 2.7591876208897488e-05,
      "loss": 1.4755,
      "step": 1350
    },
    {
      "epoch": 0.8968609865470852,
      "grad_norm": 8.205131530761719,
      "learning_rate": 2.7495164410058027e-05,
      "loss": 1.5551,
      "step": 1400
    },
    {
      "epoch": 0.928891736066624,
      "grad_norm": 6.660746097564697,
      "learning_rate": 2.739845261121857e-05,
      "loss": 1.5081,
      "step": 1450
    },
    {
      "epoch": 0.9609224855861627,
      "grad_norm": 6.179211616516113,
      "learning_rate": 2.730174081237911e-05,
      "loss": 1.5289,
      "step": 1500
    },
    {
      "epoch": 0.9929532351057014,
      "grad_norm": 7.85088586807251,
      "learning_rate": 2.7205029013539653e-05,
      "loss": 1.3848,
      "step": 1550
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.299912929534912,
      "eval_runtime": 2.367,
      "eval_samples_per_second": 293.193,
      "eval_steps_per_second": 73.51,
      "step": 1561
    },
    {
      "epoch": 1.0249839846252402,
      "grad_norm": 5.733024597167969,
      "learning_rate": 2.7108317214700195e-05,
      "loss": 1.401,
      "step": 1600
    },
    {
      "epoch": 1.057014734144779,
      "grad_norm": 5.223918437957764,
      "learning_rate": 2.7011605415860737e-05,
      "loss": 1.2967,
      "step": 1650
    },
    {
      "epoch": 1.0890454836643177,
      "grad_norm": 5.694051265716553,
      "learning_rate": 2.691489361702128e-05,
      "loss": 1.2662,
      "step": 1700
    },
    {
      "epoch": 1.1210762331838564,
      "grad_norm": 6.579220771789551,
      "learning_rate": 2.6818181818181817e-05,
      "loss": 1.29,
      "step": 1750
    },
    {
      "epoch": 1.1531069827033953,
      "grad_norm": 5.207971096038818,
      "learning_rate": 2.672147001934236e-05,
      "loss": 1.3122,
      "step": 1800
    },
    {
      "epoch": 1.185137732222934,
      "grad_norm": 4.232680797576904,
      "learning_rate": 2.66247582205029e-05,
      "loss": 1.2664,
      "step": 1850
    },
    {
      "epoch": 1.2171684817424728,
      "grad_norm": 10.237183570861816,
      "learning_rate": 2.6528046421663443e-05,
      "loss": 1.2921,
      "step": 1900
    },
    {
      "epoch": 1.2491992312620115,
      "grad_norm": 6.520907878875732,
      "learning_rate": 2.6431334622823985e-05,
      "loss": 1.2352,
      "step": 1950
    },
    {
      "epoch": 1.2812299807815504,
      "grad_norm": 4.704708576202393,
      "learning_rate": 2.6334622823984527e-05,
      "loss": 1.2843,
      "step": 2000
    },
    {
      "epoch": 1.313260730301089,
      "grad_norm": 9.011486053466797,
      "learning_rate": 2.623791102514507e-05,
      "loss": 1.3256,
      "step": 2050
    },
    {
      "epoch": 1.3452914798206277,
      "grad_norm": 4.7927985191345215,
      "learning_rate": 2.6141199226305608e-05,
      "loss": 1.3038,
      "step": 2100
    },
    {
      "epoch": 1.3773222293401666,
      "grad_norm": 8.485547065734863,
      "learning_rate": 2.6044487427466153e-05,
      "loss": 1.2163,
      "step": 2150
    },
    {
      "epoch": 1.4093529788597055,
      "grad_norm": 9.581151008605957,
      "learning_rate": 2.5947775628626692e-05,
      "loss": 1.2319,
      "step": 2200
    },
    {
      "epoch": 1.4413837283792441,
      "grad_norm": 4.167097568511963,
      "learning_rate": 2.5851063829787234e-05,
      "loss": 1.2177,
      "step": 2250
    },
    {
      "epoch": 1.4734144778987828,
      "grad_norm": 5.209082126617432,
      "learning_rate": 2.575435203094778e-05,
      "loss": 1.2678,
      "step": 2300
    },
    {
      "epoch": 1.5054452274183214,
      "grad_norm": 4.594429016113281,
      "learning_rate": 2.5657640232108318e-05,
      "loss": 1.1889,
      "step": 2350
    },
    {
      "epoch": 1.5374759769378603,
      "grad_norm": 4.982265472412109,
      "learning_rate": 2.556092843326886e-05,
      "loss": 1.277,
      "step": 2400
    },
    {
      "epoch": 1.5695067264573992,
      "grad_norm": 6.031889915466309,
      "learning_rate": 2.5464216634429402e-05,
      "loss": 1.2659,
      "step": 2450
    },
    {
      "epoch": 1.6015374759769379,
      "grad_norm": 6.042058944702148,
      "learning_rate": 2.5367504835589944e-05,
      "loss": 1.3073,
      "step": 2500
    },
    {
      "epoch": 1.6335682254964765,
      "grad_norm": 5.09761905670166,
      "learning_rate": 2.5270793036750482e-05,
      "loss": 1.261,
      "step": 2550
    },
    {
      "epoch": 1.6655989750160154,
      "grad_norm": 5.200177192687988,
      "learning_rate": 2.5174081237911028e-05,
      "loss": 1.2156,
      "step": 2600
    },
    {
      "epoch": 1.6976297245355543,
      "grad_norm": 4.705789089202881,
      "learning_rate": 2.507736943907157e-05,
      "loss": 1.2543,
      "step": 2650
    },
    {
      "epoch": 1.729660474055093,
      "grad_norm": 6.122701644897461,
      "learning_rate": 2.4980657640232108e-05,
      "loss": 1.2583,
      "step": 2700
    },
    {
      "epoch": 1.7616912235746316,
      "grad_norm": 6.770651817321777,
      "learning_rate": 2.488394584139265e-05,
      "loss": 1.2981,
      "step": 2750
    },
    {
      "epoch": 1.7937219730941703,
      "grad_norm": 3.9787216186523438,
      "learning_rate": 2.4787234042553192e-05,
      "loss": 1.2613,
      "step": 2800
    },
    {
      "epoch": 1.8257527226137091,
      "grad_norm": 4.336219787597656,
      "learning_rate": 2.4690522243713734e-05,
      "loss": 1.2337,
      "step": 2850
    },
    {
      "epoch": 1.857783472133248,
      "grad_norm": 6.146962642669678,
      "learning_rate": 2.4593810444874273e-05,
      "loss": 1.2488,
      "step": 2900
    },
    {
      "epoch": 1.8898142216527867,
      "grad_norm": 4.183321952819824,
      "learning_rate": 2.4497098646034818e-05,
      "loss": 1.2381,
      "step": 2950
    },
    {
      "epoch": 1.9218449711723253,
      "grad_norm": 5.972630977630615,
      "learning_rate": 2.4400386847195357e-05,
      "loss": 1.2355,
      "step": 3000
    },
    {
      "epoch": 1.9538757206918642,
      "grad_norm": 4.2166314125061035,
      "learning_rate": 2.43036750483559e-05,
      "loss": 1.253,
      "step": 3050
    },
    {
      "epoch": 1.985906470211403,
      "grad_norm": 3.983407735824585,
      "learning_rate": 2.4206963249516444e-05,
      "loss": 1.2714,
      "step": 3100
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.2455780506134033,
      "eval_runtime": 2.3405,
      "eval_samples_per_second": 296.518,
      "eval_steps_per_second": 74.343,
      "step": 3122
    },
    {
      "epoch": 2.0179372197309418,
      "grad_norm": 5.3564300537109375,
      "learning_rate": 2.4110251450676983e-05,
      "loss": 1.2159,
      "step": 3150
    },
    {
      "epoch": 2.0499679692504804,
      "grad_norm": 8.016033172607422,
      "learning_rate": 2.4013539651837525e-05,
      "loss": 1.0755,
      "step": 3200
    },
    {
      "epoch": 2.081998718770019,
      "grad_norm": 5.634252071380615,
      "learning_rate": 2.3916827852998067e-05,
      "loss": 1.0924,
      "step": 3250
    },
    {
      "epoch": 2.114029468289558,
      "grad_norm": 5.819911479949951,
      "learning_rate": 2.382011605415861e-05,
      "loss": 1.075,
      "step": 3300
    },
    {
      "epoch": 2.146060217809097,
      "grad_norm": 3.899829387664795,
      "learning_rate": 2.3723404255319147e-05,
      "loss": 1.1145,
      "step": 3350
    },
    {
      "epoch": 2.1780909673286355,
      "grad_norm": 5.2994256019592285,
      "learning_rate": 2.3626692456479693e-05,
      "loss": 1.0764,
      "step": 3400
    },
    {
      "epoch": 2.210121716848174,
      "grad_norm": 6.590281009674072,
      "learning_rate": 2.3529980657640235e-05,
      "loss": 1.0757,
      "step": 3450
    },
    {
      "epoch": 2.242152466367713,
      "grad_norm": 5.696014881134033,
      "learning_rate": 2.3433268858800773e-05,
      "loss": 1.1097,
      "step": 3500
    },
    {
      "epoch": 2.274183215887252,
      "grad_norm": 4.489586353302002,
      "learning_rate": 2.3336557059961315e-05,
      "loss": 1.1111,
      "step": 3550
    },
    {
      "epoch": 2.3062139654067906,
      "grad_norm": 4.391184329986572,
      "learning_rate": 2.3239845261121857e-05,
      "loss": 1.0736,
      "step": 3600
    },
    {
      "epoch": 2.3382447149263292,
      "grad_norm": 5.07828950881958,
      "learning_rate": 2.31431334622824e-05,
      "loss": 1.0808,
      "step": 3650
    },
    {
      "epoch": 2.370275464445868,
      "grad_norm": 5.394570350646973,
      "learning_rate": 2.3046421663442938e-05,
      "loss": 1.0786,
      "step": 3700
    },
    {
      "epoch": 2.4023062139654066,
      "grad_norm": 5.389923095703125,
      "learning_rate": 2.2949709864603483e-05,
      "loss": 1.1566,
      "step": 3750
    },
    {
      "epoch": 2.4343369634849457,
      "grad_norm": 5.235396862030029,
      "learning_rate": 2.2852998065764025e-05,
      "loss": 1.1675,
      "step": 3800
    },
    {
      "epoch": 2.4663677130044843,
      "grad_norm": 6.091310977935791,
      "learning_rate": 2.2756286266924564e-05,
      "loss": 1.0501,
      "step": 3850
    },
    {
      "epoch": 2.498398462524023,
      "grad_norm": 3.6405348777770996,
      "learning_rate": 2.265957446808511e-05,
      "loss": 1.1071,
      "step": 3900
    },
    {
      "epoch": 2.530429212043562,
      "grad_norm": 5.498761177062988,
      "learning_rate": 2.2562862669245648e-05,
      "loss": 1.0984,
      "step": 3950
    },
    {
      "epoch": 2.5624599615631007,
      "grad_norm": 6.096620082855225,
      "learning_rate": 2.246615087040619e-05,
      "loss": 1.0477,
      "step": 4000
    },
    {
      "epoch": 2.5944907110826394,
      "grad_norm": 6.229475975036621,
      "learning_rate": 2.236943907156673e-05,
      "loss": 1.0599,
      "step": 4050
    },
    {
      "epoch": 2.626521460602178,
      "grad_norm": 6.087823390960693,
      "learning_rate": 2.2272727272727274e-05,
      "loss": 1.0648,
      "step": 4100
    },
    {
      "epoch": 2.6585522101217167,
      "grad_norm": 6.665729999542236,
      "learning_rate": 2.2176015473887816e-05,
      "loss": 1.1273,
      "step": 4150
    },
    {
      "epoch": 2.6905829596412554,
      "grad_norm": 3.9531400203704834,
      "learning_rate": 2.2079303675048358e-05,
      "loss": 0.9798,
      "step": 4200
    },
    {
      "epoch": 2.7226137091607945,
      "grad_norm": 7.884270668029785,
      "learning_rate": 2.19825918762089e-05,
      "loss": 1.1014,
      "step": 4250
    },
    {
      "epoch": 2.754644458680333,
      "grad_norm": 5.26248025894165,
      "learning_rate": 2.1885880077369438e-05,
      "loss": 1.0787,
      "step": 4300
    },
    {
      "epoch": 2.786675208199872,
      "grad_norm": 6.6773271560668945,
      "learning_rate": 2.1789168278529983e-05,
      "loss": 1.0763,
      "step": 4350
    },
    {
      "epoch": 2.818705957719411,
      "grad_norm": 7.597923755645752,
      "learning_rate": 2.1692456479690522e-05,
      "loss": 1.1088,
      "step": 4400
    },
    {
      "epoch": 2.8507367072389496,
      "grad_norm": 7.87522029876709,
      "learning_rate": 2.1595744680851064e-05,
      "loss": 1.0751,
      "step": 4450
    },
    {
      "epoch": 2.8827674567584882,
      "grad_norm": 6.8779988288879395,
      "learning_rate": 2.1500967117988392e-05,
      "loss": 1.1419,
      "step": 4500
    },
    {
      "epoch": 2.914798206278027,
      "grad_norm": 4.711526393890381,
      "learning_rate": 2.1404255319148938e-05,
      "loss": 1.18,
      "step": 4550
    },
    {
      "epoch": 2.9468289557975655,
      "grad_norm": 5.468163013458252,
      "learning_rate": 2.130754352030948e-05,
      "loss": 1.1811,
      "step": 4600
    },
    {
      "epoch": 2.978859705317104,
      "grad_norm": 5.626739025115967,
      "learning_rate": 2.1210831721470018e-05,
      "loss": 1.1063,
      "step": 4650
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.2298176288604736,
      "eval_runtime": 2.3418,
      "eval_samples_per_second": 296.35,
      "eval_steps_per_second": 74.301,
      "step": 4683
    },
    {
      "epoch": 3.0108904548366433,
      "grad_norm": 4.209212303161621,
      "learning_rate": 2.1114119922630564e-05,
      "loss": 1.0208,
      "step": 4700
    },
    {
      "epoch": 3.042921204356182,
      "grad_norm": 3.9859347343444824,
      "learning_rate": 2.1017408123791102e-05,
      "loss": 0.8951,
      "step": 4750
    },
    {
      "epoch": 3.0749519538757206,
      "grad_norm": 4.6022796630859375,
      "learning_rate": 2.0920696324951644e-05,
      "loss": 0.972,
      "step": 4800
    },
    {
      "epoch": 3.1069827033952593,
      "grad_norm": 3.8590571880340576,
      "learning_rate": 2.0823984526112186e-05,
      "loss": 0.8733,
      "step": 4850
    },
    {
      "epoch": 3.1390134529147984,
      "grad_norm": 5.990377426147461,
      "learning_rate": 2.0727272727272728e-05,
      "loss": 0.9669,
      "step": 4900
    },
    {
      "epoch": 3.171044202434337,
      "grad_norm": 6.3074870109558105,
      "learning_rate": 2.063056092843327e-05,
      "loss": 0.9315,
      "step": 4950
    },
    {
      "epoch": 3.2030749519538757,
      "grad_norm": 4.565438747406006,
      "learning_rate": 2.0533849129593812e-05,
      "loss": 1.0512,
      "step": 5000
    },
    {
      "epoch": 3.2351057014734144,
      "grad_norm": 6.703009605407715,
      "learning_rate": 2.0437137330754354e-05,
      "loss": 0.9842,
      "step": 5050
    },
    {
      "epoch": 3.267136450992953,
      "grad_norm": 5.883960247039795,
      "learning_rate": 2.0340425531914893e-05,
      "loss": 0.845,
      "step": 5100
    },
    {
      "epoch": 3.299167200512492,
      "grad_norm": 4.492222309112549,
      "learning_rate": 2.0243713733075438e-05,
      "loss": 1.0016,
      "step": 5150
    },
    {
      "epoch": 3.331197950032031,
      "grad_norm": 5.360101222991943,
      "learning_rate": 2.0147001934235977e-05,
      "loss": 1.0232,
      "step": 5200
    },
    {
      "epoch": 3.3632286995515694,
      "grad_norm": 4.394837379455566,
      "learning_rate": 2.005029013539652e-05,
      "loss": 1.007,
      "step": 5250
    },
    {
      "epoch": 3.395259449071108,
      "grad_norm": 4.638077259063721,
      "learning_rate": 1.995357833655706e-05,
      "loss": 0.957,
      "step": 5300
    },
    {
      "epoch": 3.427290198590647,
      "grad_norm": 5.213028430938721,
      "learning_rate": 1.9856866537717603e-05,
      "loss": 0.9554,
      "step": 5350
    },
    {
      "epoch": 3.459320948110186,
      "grad_norm": 7.427182197570801,
      "learning_rate": 1.9760154738878145e-05,
      "loss": 0.9467,
      "step": 5400
    },
    {
      "epoch": 3.4913516976297245,
      "grad_norm": 5.193233966827393,
      "learning_rate": 1.9663442940038683e-05,
      "loss": 0.9798,
      "step": 5450
    },
    {
      "epoch": 3.523382447149263,
      "grad_norm": 4.17158317565918,
      "learning_rate": 1.956673114119923e-05,
      "loss": 0.9437,
      "step": 5500
    },
    {
      "epoch": 3.555413196668802,
      "grad_norm": 5.318813800811768,
      "learning_rate": 1.9470019342359767e-05,
      "loss": 0.9243,
      "step": 5550
    },
    {
      "epoch": 3.587443946188341,
      "grad_norm": 5.195188999176025,
      "learning_rate": 1.937330754352031e-05,
      "loss": 0.9887,
      "step": 5600
    },
    {
      "epoch": 3.6194746957078796,
      "grad_norm": 4.908803462982178,
      "learning_rate": 1.927659574468085e-05,
      "loss": 0.9731,
      "step": 5650
    },
    {
      "epoch": 3.6515054452274183,
      "grad_norm": 4.569502830505371,
      "learning_rate": 1.9179883945841393e-05,
      "loss": 0.9217,
      "step": 5700
    },
    {
      "epoch": 3.683536194746957,
      "grad_norm": 4.746579170227051,
      "learning_rate": 1.9083172147001935e-05,
      "loss": 0.9718,
      "step": 5750
    },
    {
      "epoch": 3.715566944266496,
      "grad_norm": 4.926259994506836,
      "learning_rate": 1.8986460348162477e-05,
      "loss": 1.0328,
      "step": 5800
    },
    {
      "epoch": 3.7475976937860347,
      "grad_norm": 5.11845064163208,
      "learning_rate": 1.888974854932302e-05,
      "loss": 0.9318,
      "step": 5850
    },
    {
      "epoch": 3.7796284433055733,
      "grad_norm": 7.5701212882995605,
      "learning_rate": 1.8793036750483558e-05,
      "loss": 1.0465,
      "step": 5900
    },
    {
      "epoch": 3.811659192825112,
      "grad_norm": 5.804500102996826,
      "learning_rate": 1.8696324951644103e-05,
      "loss": 1.0048,
      "step": 5950
    },
    {
      "epoch": 3.8436899423446507,
      "grad_norm": 7.772210121154785,
      "learning_rate": 1.859961315280464e-05,
      "loss": 1.0134,
      "step": 6000
    },
    {
      "epoch": 3.8757206918641898,
      "grad_norm": 5.380468368530273,
      "learning_rate": 1.8502901353965184e-05,
      "loss": 1.0546,
      "step": 6050
    },
    {
      "epoch": 3.9077514413837284,
      "grad_norm": 5.766050815582275,
      "learning_rate": 1.8406189555125725e-05,
      "loss": 0.9031,
      "step": 6100
    },
    {
      "epoch": 3.939782190903267,
      "grad_norm": 4.941691875457764,
      "learning_rate": 1.8309477756286267e-05,
      "loss": 1.0435,
      "step": 6150
    },
    {
      "epoch": 3.9718129404228057,
      "grad_norm": 4.834738731384277,
      "learning_rate": 1.821276595744681e-05,
      "loss": 1.0757,
      "step": 6200
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.2243226766586304,
      "eval_runtime": 2.3641,
      "eval_samples_per_second": 293.555,
      "eval_steps_per_second": 73.6,
      "step": 6244
    },
    {
      "epoch": 4.003843689942345,
      "grad_norm": 5.316913604736328,
      "learning_rate": 1.8116054158607348e-05,
      "loss": 0.9772,
      "step": 6250
    },
    {
      "epoch": 4.0358744394618835,
      "grad_norm": 4.583508014678955,
      "learning_rate": 1.8019342359767893e-05,
      "loss": 0.8616,
      "step": 6300
    },
    {
      "epoch": 4.067905188981422,
      "grad_norm": 5.029387474060059,
      "learning_rate": 1.7922630560928432e-05,
      "loss": 0.8083,
      "step": 6350
    },
    {
      "epoch": 4.099935938500961,
      "grad_norm": 6.853472709655762,
      "learning_rate": 1.7825918762088974e-05,
      "loss": 0.8142,
      "step": 6400
    },
    {
      "epoch": 4.1319666880204995,
      "grad_norm": 7.233567237854004,
      "learning_rate": 1.772920696324952e-05,
      "loss": 0.8747,
      "step": 6450
    },
    {
      "epoch": 4.163997437540038,
      "grad_norm": 8.294999122619629,
      "learning_rate": 1.7632495164410058e-05,
      "loss": 0.8707,
      "step": 6500
    },
    {
      "epoch": 4.196028187059577,
      "grad_norm": 5.9975104331970215,
      "learning_rate": 1.75357833655706e-05,
      "loss": 0.8616,
      "step": 6550
    },
    {
      "epoch": 4.228058936579116,
      "grad_norm": 3.878993511199951,
      "learning_rate": 1.7439071566731142e-05,
      "loss": 0.8768,
      "step": 6600
    },
    {
      "epoch": 4.260089686098655,
      "grad_norm": 4.366540908813477,
      "learning_rate": 1.7342359767891684e-05,
      "loss": 0.8574,
      "step": 6650
    },
    {
      "epoch": 4.292120435618194,
      "grad_norm": 5.50658655166626,
      "learning_rate": 1.7245647969052222e-05,
      "loss": 0.8309,
      "step": 6700
    },
    {
      "epoch": 4.324151185137732,
      "grad_norm": 3.9399383068084717,
      "learning_rate": 1.7148936170212768e-05,
      "loss": 0.8539,
      "step": 6750
    },
    {
      "epoch": 4.356181934657271,
      "grad_norm": 6.372839450836182,
      "learning_rate": 1.705222437137331e-05,
      "loss": 0.941,
      "step": 6800
    },
    {
      "epoch": 4.38821268417681,
      "grad_norm": 5.761376857757568,
      "learning_rate": 1.695551257253385e-05,
      "loss": 0.9164,
      "step": 6850
    },
    {
      "epoch": 4.420243433696348,
      "grad_norm": 6.147214412689209,
      "learning_rate": 1.6858800773694394e-05,
      "loss": 0.8923,
      "step": 6900
    },
    {
      "epoch": 4.452274183215887,
      "grad_norm": 4.454633712768555,
      "learning_rate": 1.6762088974854932e-05,
      "loss": 0.8837,
      "step": 6950
    },
    {
      "epoch": 4.484304932735426,
      "grad_norm": 5.585158348083496,
      "learning_rate": 1.6665377176015474e-05,
      "loss": 0.8681,
      "step": 7000
    },
    {
      "epoch": 4.516335682254965,
      "grad_norm": 4.092731952667236,
      "learning_rate": 1.6568665377176013e-05,
      "loss": 0.8361,
      "step": 7050
    },
    {
      "epoch": 4.548366431774504,
      "grad_norm": 4.714837074279785,
      "learning_rate": 1.647195357833656e-05,
      "loss": 0.8936,
      "step": 7100
    },
    {
      "epoch": 4.5803971812940425,
      "grad_norm": 5.136377334594727,
      "learning_rate": 1.63752417794971e-05,
      "loss": 0.8953,
      "step": 7150
    },
    {
      "epoch": 4.612427930813581,
      "grad_norm": 5.447563171386719,
      "learning_rate": 1.627852998065764e-05,
      "loss": 0.8513,
      "step": 7200
    },
    {
      "epoch": 4.64445868033312,
      "grad_norm": 5.151524066925049,
      "learning_rate": 1.6181818181818184e-05,
      "loss": 0.845,
      "step": 7250
    },
    {
      "epoch": 4.6764894298526585,
      "grad_norm": 3.8443634510040283,
      "learning_rate": 1.6085106382978723e-05,
      "loss": 0.8902,
      "step": 7300
    },
    {
      "epoch": 4.708520179372197,
      "grad_norm": 4.044720649719238,
      "learning_rate": 1.5988394584139265e-05,
      "loss": 0.8745,
      "step": 7350
    },
    {
      "epoch": 4.740550928891736,
      "grad_norm": 4.071728229522705,
      "learning_rate": 1.5891682785299807e-05,
      "loss": 0.86,
      "step": 7400
    },
    {
      "epoch": 4.7725816784112745,
      "grad_norm": 5.550813674926758,
      "learning_rate": 1.579497098646035e-05,
      "loss": 0.903,
      "step": 7450
    },
    {
      "epoch": 4.804612427930813,
      "grad_norm": 6.840596675872803,
      "learning_rate": 1.569825918762089e-05,
      "loss": 0.8735,
      "step": 7500
    },
    {
      "epoch": 4.836643177450353,
      "grad_norm": 5.488367557525635,
      "learning_rate": 1.5601547388781433e-05,
      "loss": 0.8716,
      "step": 7550
    },
    {
      "epoch": 4.868673926969891,
      "grad_norm": 4.531365871429443,
      "learning_rate": 1.5504835589941975e-05,
      "loss": 0.844,
      "step": 7600
    },
    {
      "epoch": 4.90070467648943,
      "grad_norm": 5.380832672119141,
      "learning_rate": 1.5408123791102513e-05,
      "loss": 0.9107,
      "step": 7650
    },
    {
      "epoch": 4.932735426008969,
      "grad_norm": 6.288106441497803,
      "learning_rate": 1.531141199226306e-05,
      "loss": 0.8945,
      "step": 7700
    },
    {
      "epoch": 4.964766175528507,
      "grad_norm": 4.873928546905518,
      "learning_rate": 1.5214700193423599e-05,
      "loss": 0.8906,
      "step": 7750
    },
    {
      "epoch": 4.996796925048046,
      "grad_norm": 3.991455316543579,
      "learning_rate": 1.511798839458414e-05,
      "loss": 0.8131,
      "step": 7800
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.251530647277832,
      "eval_runtime": 2.3436,
      "eval_samples_per_second": 296.126,
      "eval_steps_per_second": 74.245,
      "step": 7805
    },
    {
      "epoch": 5.028827674567585,
      "grad_norm": 5.143444061279297,
      "learning_rate": 1.502127659574468e-05,
      "loss": 0.744,
      "step": 7850
    },
    {
      "epoch": 5.060858424087123,
      "grad_norm": 7.357913494110107,
      "learning_rate": 1.4924564796905223e-05,
      "loss": 0.7627,
      "step": 7900
    },
    {
      "epoch": 5.092889173606663,
      "grad_norm": 6.090378284454346,
      "learning_rate": 1.4827852998065764e-05,
      "loss": 0.7593,
      "step": 7950
    },
    {
      "epoch": 5.1249199231262015,
      "grad_norm": 5.816789150238037,
      "learning_rate": 1.4731141199226306e-05,
      "loss": 0.7705,
      "step": 8000
    },
    {
      "epoch": 5.15695067264574,
      "grad_norm": 4.871501445770264,
      "learning_rate": 1.4634429400386848e-05,
      "loss": 0.8032,
      "step": 8050
    },
    {
      "epoch": 5.188981422165279,
      "grad_norm": 4.780886650085449,
      "learning_rate": 1.453771760154739e-05,
      "loss": 0.7434,
      "step": 8100
    },
    {
      "epoch": 5.2210121716848175,
      "grad_norm": 4.641584396362305,
      "learning_rate": 1.4441005802707931e-05,
      "loss": 0.7943,
      "step": 8150
    },
    {
      "epoch": 5.253042921204356,
      "grad_norm": 5.049288749694824,
      "learning_rate": 1.4344294003868472e-05,
      "loss": 0.8028,
      "step": 8200
    },
    {
      "epoch": 5.285073670723895,
      "grad_norm": 5.066954612731934,
      "learning_rate": 1.4247582205029014e-05,
      "loss": 0.7909,
      "step": 8250
    },
    {
      "epoch": 5.317104420243433,
      "grad_norm": 7.368260383605957,
      "learning_rate": 1.4150870406189556e-05,
      "loss": 0.7887,
      "step": 8300
    },
    {
      "epoch": 5.349135169762972,
      "grad_norm": 5.071633338928223,
      "learning_rate": 1.4054158607350096e-05,
      "loss": 0.7804,
      "step": 8350
    },
    {
      "epoch": 5.381165919282511,
      "grad_norm": 4.657227516174316,
      "learning_rate": 1.3957446808510638e-05,
      "loss": 0.8269,
      "step": 8400
    },
    {
      "epoch": 5.41319666880205,
      "grad_norm": 5.885330677032471,
      "learning_rate": 1.3860735009671182e-05,
      "loss": 0.7045,
      "step": 8450
    },
    {
      "epoch": 5.445227418321589,
      "grad_norm": 5.427860736846924,
      "learning_rate": 1.3764023210831722e-05,
      "loss": 0.7946,
      "step": 8500
    },
    {
      "epoch": 5.477258167841128,
      "grad_norm": 4.647580146789551,
      "learning_rate": 1.3667311411992264e-05,
      "loss": 0.7644,
      "step": 8550
    },
    {
      "epoch": 5.509288917360666,
      "grad_norm": 6.313737392425537,
      "learning_rate": 1.3570599613152804e-05,
      "loss": 0.7954,
      "step": 8600
    },
    {
      "epoch": 5.541319666880205,
      "grad_norm": 8.128386497497559,
      "learning_rate": 1.3473887814313346e-05,
      "loss": 0.7943,
      "step": 8650
    },
    {
      "epoch": 5.573350416399744,
      "grad_norm": 5.885477542877197,
      "learning_rate": 1.3377176015473888e-05,
      "loss": 0.7485,
      "step": 8700
    },
    {
      "epoch": 5.605381165919282,
      "grad_norm": 4.388980388641357,
      "learning_rate": 1.3280464216634428e-05,
      "loss": 0.8386,
      "step": 8750
    },
    {
      "epoch": 5.637411915438821,
      "grad_norm": 5.510167598724365,
      "learning_rate": 1.3183752417794972e-05,
      "loss": 0.7934,
      "step": 8800
    },
    {
      "epoch": 5.6694426649583605,
      "grad_norm": 4.3247270584106445,
      "learning_rate": 1.3087040618955514e-05,
      "loss": 0.8104,
      "step": 8850
    },
    {
      "epoch": 5.701473414477899,
      "grad_norm": 5.528905868530273,
      "learning_rate": 1.2990328820116054e-05,
      "loss": 0.7575,
      "step": 8900
    },
    {
      "epoch": 5.733504163997438,
      "grad_norm": 5.551684379577637,
      "learning_rate": 1.2893617021276596e-05,
      "loss": 0.7955,
      "step": 8950
    },
    {
      "epoch": 5.7655349135169764,
      "grad_norm": 6.043674468994141,
      "learning_rate": 1.2796905222437137e-05,
      "loss": 0.8328,
      "step": 9000
    },
    {
      "epoch": 5.797565663036515,
      "grad_norm": 6.115534782409668,
      "learning_rate": 1.2700193423597679e-05,
      "loss": 0.8287,
      "step": 9050
    },
    {
      "epoch": 5.829596412556054,
      "grad_norm": 4.203439712524414,
      "learning_rate": 1.260348162475822e-05,
      "loss": 0.8094,
      "step": 9100
    },
    {
      "epoch": 5.861627162075592,
      "grad_norm": 5.491913795471191,
      "learning_rate": 1.2506769825918763e-05,
      "loss": 0.8162,
      "step": 9150
    },
    {
      "epoch": 5.893657911595131,
      "grad_norm": 5.7638397216796875,
      "learning_rate": 1.2410058027079305e-05,
      "loss": 0.8317,
      "step": 9200
    },
    {
      "epoch": 5.92568866111467,
      "grad_norm": 6.701140880584717,
      "learning_rate": 1.2313346228239847e-05,
      "loss": 0.8361,
      "step": 9250
    },
    {
      "epoch": 5.957719410634208,
      "grad_norm": 4.590863227844238,
      "learning_rate": 1.2216634429400387e-05,
      "loss": 0.7756,
      "step": 9300
    },
    {
      "epoch": 5.989750160153748,
      "grad_norm": 6.663580894470215,
      "learning_rate": 1.2119922630560929e-05,
      "loss": 0.817,
      "step": 9350
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.2634634971618652,
      "eval_runtime": 2.3436,
      "eval_samples_per_second": 296.13,
      "eval_steps_per_second": 74.246,
      "step": 9366
    },
    {
      "epoch": 6.021780909673287,
      "grad_norm": 5.906644821166992,
      "learning_rate": 1.2023210831721469e-05,
      "loss": 0.7228,
      "step": 9400
    },
    {
      "epoch": 6.053811659192825,
      "grad_norm": 5.922998428344727,
      "learning_rate": 1.1926499032882011e-05,
      "loss": 0.7092,
      "step": 9450
    },
    {
      "epoch": 6.085842408712364,
      "grad_norm": 4.2865424156188965,
      "learning_rate": 1.1829787234042553e-05,
      "loss": 0.7386,
      "step": 9500
    },
    {
      "epoch": 6.117873158231903,
      "grad_norm": 4.481253147125244,
      "learning_rate": 1.1733075435203095e-05,
      "loss": 0.7029,
      "step": 9550
    },
    {
      "epoch": 6.149903907751441,
      "grad_norm": 4.568172931671143,
      "learning_rate": 1.1636363636363637e-05,
      "loss": 0.7182,
      "step": 9600
    },
    {
      "epoch": 6.18193465727098,
      "grad_norm": 6.614755153656006,
      "learning_rate": 1.1539651837524179e-05,
      "loss": 0.7327,
      "step": 9650
    },
    {
      "epoch": 6.213965406790519,
      "grad_norm": 4.054879665374756,
      "learning_rate": 1.144294003868472e-05,
      "loss": 0.6989,
      "step": 9700
    },
    {
      "epoch": 6.245996156310058,
      "grad_norm": 5.12107515335083,
      "learning_rate": 1.1346228239845261e-05,
      "loss": 0.6814,
      "step": 9750
    },
    {
      "epoch": 6.278026905829597,
      "grad_norm": 4.803263187408447,
      "learning_rate": 1.1249516441005803e-05,
      "loss": 0.7289,
      "step": 9800
    },
    {
      "epoch": 6.310057655349135,
      "grad_norm": 5.340466022491455,
      "learning_rate": 1.1152804642166344e-05,
      "loss": 0.7514,
      "step": 9850
    },
    {
      "epoch": 6.342088404868674,
      "grad_norm": 5.041305065155029,
      "learning_rate": 1.1056092843326887e-05,
      "loss": 0.7618,
      "step": 9900
    },
    {
      "epoch": 6.374119154388213,
      "grad_norm": 4.7344970703125,
      "learning_rate": 1.0959381044487428e-05,
      "loss": 0.7828,
      "step": 9950
    },
    {
      "epoch": 6.406149903907751,
      "grad_norm": 5.425915718078613,
      "learning_rate": 1.086266924564797e-05,
      "loss": 0.7184,
      "step": 10000
    },
    {
      "epoch": 6.43818065342729,
      "grad_norm": 5.80362606048584,
      "learning_rate": 1.0765957446808512e-05,
      "loss": 0.7635,
      "step": 10050
    },
    {
      "epoch": 6.470211402946829,
      "grad_norm": 5.3292083740234375,
      "learning_rate": 1.0669245647969052e-05,
      "loss": 0.7511,
      "step": 10100
    },
    {
      "epoch": 6.502242152466367,
      "grad_norm": 7.044926643371582,
      "learning_rate": 1.0572533849129594e-05,
      "loss": 0.7198,
      "step": 10150
    },
    {
      "epoch": 6.534272901985906,
      "grad_norm": 8.361040115356445,
      "learning_rate": 1.0475822050290136e-05,
      "loss": 0.7165,
      "step": 10200
    },
    {
      "epoch": 6.566303651505446,
      "grad_norm": 4.531733989715576,
      "learning_rate": 1.0379110251450678e-05,
      "loss": 0.705,
      "step": 10250
    },
    {
      "epoch": 6.598334401024984,
      "grad_norm": 5.874772548675537,
      "learning_rate": 1.028239845261122e-05,
      "loss": 0.7207,
      "step": 10300
    },
    {
      "epoch": 6.630365150544523,
      "grad_norm": 5.760225296020508,
      "learning_rate": 1.018568665377176e-05,
      "loss": 0.7263,
      "step": 10350
    },
    {
      "epoch": 6.662395900064062,
      "grad_norm": 5.511386871337891,
      "learning_rate": 1.0088974854932302e-05,
      "loss": 0.7371,
      "step": 10400
    },
    {
      "epoch": 6.6944266495836,
      "grad_norm": 4.946433067321777,
      "learning_rate": 9.992263056092844e-06,
      "loss": 0.7378,
      "step": 10450
    },
    {
      "epoch": 6.726457399103139,
      "grad_norm": 3.938204765319824,
      "learning_rate": 9.895551257253384e-06,
      "loss": 0.745,
      "step": 10500
    },
    {
      "epoch": 6.7584881486226775,
      "grad_norm": 4.752959728240967,
      "learning_rate": 9.798839458413926e-06,
      "loss": 0.743,
      "step": 10550
    },
    {
      "epoch": 6.790518898142216,
      "grad_norm": 3.488236904144287,
      "learning_rate": 9.702127659574468e-06,
      "loss": 0.7605,
      "step": 10600
    },
    {
      "epoch": 6.822549647661756,
      "grad_norm": 7.006624698638916,
      "learning_rate": 9.60541586073501e-06,
      "loss": 0.6636,
      "step": 10650
    },
    {
      "epoch": 6.854580397181294,
      "grad_norm": 5.008205890655518,
      "learning_rate": 9.508704061895552e-06,
      "loss": 0.7068,
      "step": 10700
    },
    {
      "epoch": 6.886611146700833,
      "grad_norm": 5.1841139793396,
      "learning_rate": 9.411992263056092e-06,
      "loss": 0.6925,
      "step": 10750
    },
    {
      "epoch": 6.918641896220372,
      "grad_norm": 6.734640598297119,
      "learning_rate": 9.315280464216634e-06,
      "loss": 0.7385,
      "step": 10800
    },
    {
      "epoch": 6.95067264573991,
      "grad_norm": 5.252043724060059,
      "learning_rate": 9.218568665377176e-06,
      "loss": 0.7455,
      "step": 10850
    },
    {
      "epoch": 6.982703395259449,
      "grad_norm": 5.879339218139648,
      "learning_rate": 9.121856866537717e-06,
      "loss": 0.6873,
      "step": 10900
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.2849360704421997,
      "eval_runtime": 2.3482,
      "eval_samples_per_second": 295.545,
      "eval_steps_per_second": 74.099,
      "step": 10927
    },
    {
      "epoch": 7.014734144778988,
      "grad_norm": 3.4435696601867676,
      "learning_rate": 9.025145067698259e-06,
      "loss": 0.6758,
      "step": 10950
    },
    {
      "epoch": 7.046764894298526,
      "grad_norm": 5.564856052398682,
      "learning_rate": 8.928433268858802e-06,
      "loss": 0.6498,
      "step": 11000
    },
    {
      "epoch": 7.078795643818065,
      "grad_norm": 3.9834165573120117,
      "learning_rate": 8.831721470019343e-06,
      "loss": 0.6313,
      "step": 11050
    },
    {
      "epoch": 7.110826393337604,
      "grad_norm": 4.025022506713867,
      "learning_rate": 8.735009671179885e-06,
      "loss": 0.6518,
      "step": 11100
    },
    {
      "epoch": 7.142857142857143,
      "grad_norm": 4.459910869598389,
      "learning_rate": 8.638297872340427e-06,
      "loss": 0.7116,
      "step": 11150
    },
    {
      "epoch": 7.174887892376682,
      "grad_norm": 4.091836929321289,
      "learning_rate": 8.541586073500967e-06,
      "loss": 0.6498,
      "step": 11200
    },
    {
      "epoch": 7.206918641896221,
      "grad_norm": 4.444058895111084,
      "learning_rate": 8.444874274661509e-06,
      "loss": 0.641,
      "step": 11250
    },
    {
      "epoch": 7.238949391415759,
      "grad_norm": 4.14004373550415,
      "learning_rate": 8.34816247582205e-06,
      "loss": 0.6697,
      "step": 11300
    },
    {
      "epoch": 7.270980140935298,
      "grad_norm": 5.868237495422363,
      "learning_rate": 8.251450676982593e-06,
      "loss": 0.6795,
      "step": 11350
    },
    {
      "epoch": 7.3030108904548365,
      "grad_norm": 4.615412712097168,
      "learning_rate": 8.154738878143135e-06,
      "loss": 0.6661,
      "step": 11400
    },
    {
      "epoch": 7.335041639974375,
      "grad_norm": 6.67661190032959,
      "learning_rate": 8.058027079303675e-06,
      "loss": 0.6132,
      "step": 11450
    },
    {
      "epoch": 7.367072389493914,
      "grad_norm": 5.743295669555664,
      "learning_rate": 7.961315280464217e-06,
      "loss": 0.6538,
      "step": 11500
    },
    {
      "epoch": 7.3991031390134525,
      "grad_norm": 4.23585844039917,
      "learning_rate": 7.864603481624759e-06,
      "loss": 0.6728,
      "step": 11550
    },
    {
      "epoch": 7.431133888532992,
      "grad_norm": 4.972036361694336,
      "learning_rate": 7.7678916827853e-06,
      "loss": 0.6425,
      "step": 11600
    },
    {
      "epoch": 7.463164638052531,
      "grad_norm": 6.330563068389893,
      "learning_rate": 7.671179883945841e-06,
      "loss": 0.6505,
      "step": 11650
    },
    {
      "epoch": 7.495195387572069,
      "grad_norm": 4.639904499053955,
      "learning_rate": 7.5744680851063825e-06,
      "loss": 0.661,
      "step": 11700
    },
    {
      "epoch": 7.527226137091608,
      "grad_norm": 5.07647180557251,
      "learning_rate": 7.4777562862669245e-06,
      "loss": 0.7185,
      "step": 11750
    },
    {
      "epoch": 7.559256886611147,
      "grad_norm": 4.9755353927612305,
      "learning_rate": 7.381044487427466e-06,
      "loss": 0.6404,
      "step": 11800
    },
    {
      "epoch": 7.591287636130685,
      "grad_norm": 5.3310394287109375,
      "learning_rate": 7.2843326885880084e-06,
      "loss": 0.6771,
      "step": 11850
    },
    {
      "epoch": 7.623318385650224,
      "grad_norm": 4.4792585372924805,
      "learning_rate": 7.1876208897485496e-06,
      "loss": 0.7089,
      "step": 11900
    },
    {
      "epoch": 7.655349135169763,
      "grad_norm": 6.186499118804932,
      "learning_rate": 7.09284332688588e-06,
      "loss": 0.6823,
      "step": 11950
    },
    {
      "epoch": 7.687379884689301,
      "grad_norm": 7.973859786987305,
      "learning_rate": 6.9961315280464215e-06,
      "loss": 0.7291,
      "step": 12000
    },
    {
      "epoch": 7.71941063420884,
      "grad_norm": 5.448941707611084,
      "learning_rate": 6.899419729206963e-06,
      "loss": 0.6774,
      "step": 12050
    },
    {
      "epoch": 7.7514413837283795,
      "grad_norm": 3.4302728176116943,
      "learning_rate": 6.8027079303675054e-06,
      "loss": 0.6643,
      "step": 12100
    },
    {
      "epoch": 7.783472133247918,
      "grad_norm": 5.240682601928711,
      "learning_rate": 6.7059961315280466e-06,
      "loss": 0.6739,
      "step": 12150
    },
    {
      "epoch": 7.815502882767457,
      "grad_norm": 5.329607009887695,
      "learning_rate": 6.609284332688588e-06,
      "loss": 0.6989,
      "step": 12200
    },
    {
      "epoch": 7.8475336322869955,
      "grad_norm": 4.242271423339844,
      "learning_rate": 6.5125725338491305e-06,
      "loss": 0.6981,
      "step": 12250
    },
    {
      "epoch": 7.879564381806534,
      "grad_norm": 4.62548303604126,
      "learning_rate": 6.415860735009672e-06,
      "loss": 0.6635,
      "step": 12300
    },
    {
      "epoch": 7.911595131326073,
      "grad_norm": 5.206060886383057,
      "learning_rate": 6.319148936170213e-06,
      "loss": 0.7124,
      "step": 12350
    },
    {
      "epoch": 7.9436258808456115,
      "grad_norm": 5.130951404571533,
      "learning_rate": 6.222437137330754e-06,
      "loss": 0.6959,
      "step": 12400
    },
    {
      "epoch": 7.97565663036515,
      "grad_norm": 5.209630012512207,
      "learning_rate": 6.125725338491297e-06,
      "loss": 0.6665,
      "step": 12450
    },
    {
      "epoch": 8.0,
      "eval_loss": 1.3043488264083862,
      "eval_runtime": 2.3343,
      "eval_samples_per_second": 297.303,
      "eval_steps_per_second": 74.54,
      "step": 12488
    },
    {
      "epoch": 8.00768737988469,
      "grad_norm": 3.8553524017333984,
      "learning_rate": 6.029013539651838e-06,
      "loss": 0.6497,
      "step": 12500
    },
    {
      "epoch": 8.039718129404228,
      "grad_norm": 5.763215065002441,
      "learning_rate": 5.932301740812379e-06,
      "loss": 0.6463,
      "step": 12550
    },
    {
      "epoch": 8.071748878923767,
      "grad_norm": 3.7186901569366455,
      "learning_rate": 5.83558994197292e-06,
      "loss": 0.6368,
      "step": 12600
    },
    {
      "epoch": 8.103779628443306,
      "grad_norm": 6.298691749572754,
      "learning_rate": 5.738878143133463e-06,
      "loss": 0.6511,
      "step": 12650
    },
    {
      "epoch": 8.135810377962844,
      "grad_norm": 6.20469856262207,
      "learning_rate": 5.642166344294004e-06,
      "loss": 0.6164,
      "step": 12700
    },
    {
      "epoch": 8.167841127482383,
      "grad_norm": 3.8918867111206055,
      "learning_rate": 5.545454545454545e-06,
      "loss": 0.6034,
      "step": 12750
    },
    {
      "epoch": 8.199871877001922,
      "grad_norm": 5.3326287269592285,
      "learning_rate": 5.448742746615087e-06,
      "loss": 0.593,
      "step": 12800
    },
    {
      "epoch": 8.23190262652146,
      "grad_norm": 7.532413959503174,
      "learning_rate": 5.352030947775629e-06,
      "loss": 0.5954,
      "step": 12850
    },
    {
      "epoch": 8.263933376040999,
      "grad_norm": 6.330820083618164,
      "learning_rate": 5.25531914893617e-06,
      "loss": 0.6214,
      "step": 12900
    },
    {
      "epoch": 8.295964125560538,
      "grad_norm": 5.831737518310547,
      "learning_rate": 5.1586073500967115e-06,
      "loss": 0.6517,
      "step": 12950
    },
    {
      "epoch": 8.327994875080076,
      "grad_norm": 3.9195139408111572,
      "learning_rate": 5.0618955512572535e-06,
      "loss": 0.605,
      "step": 13000
    },
    {
      "epoch": 8.360025624599615,
      "grad_norm": 4.626424789428711,
      "learning_rate": 4.9651837524177955e-06,
      "loss": 0.6507,
      "step": 13050
    },
    {
      "epoch": 8.392056374119154,
      "grad_norm": 5.425187110900879,
      "learning_rate": 4.868471953578337e-06,
      "loss": 0.6517,
      "step": 13100
    },
    {
      "epoch": 8.424087123638692,
      "grad_norm": 4.592123985290527,
      "learning_rate": 4.771760154738878e-06,
      "loss": 0.639,
      "step": 13150
    },
    {
      "epoch": 8.456117873158233,
      "grad_norm": 6.619301795959473,
      "learning_rate": 4.67504835589942e-06,
      "loss": 0.6108,
      "step": 13200
    },
    {
      "epoch": 8.488148622677771,
      "grad_norm": 5.363712787628174,
      "learning_rate": 4.578336557059962e-06,
      "loss": 0.6279,
      "step": 13250
    },
    {
      "epoch": 8.52017937219731,
      "grad_norm": 5.356204509735107,
      "learning_rate": 4.481624758220503e-06,
      "loss": 0.6335,
      "step": 13300
    },
    {
      "epoch": 8.552210121716849,
      "grad_norm": 7.040796756744385,
      "learning_rate": 4.384912959381045e-06,
      "loss": 0.6369,
      "step": 13350
    },
    {
      "epoch": 8.584240871236387,
      "grad_norm": 4.553722381591797,
      "learning_rate": 4.288201160541586e-06,
      "loss": 0.6432,
      "step": 13400
    },
    {
      "epoch": 8.616271620755926,
      "grad_norm": 4.4338250160217285,
      "learning_rate": 4.191489361702128e-06,
      "loss": 0.6192,
      "step": 13450
    },
    {
      "epoch": 8.648302370275465,
      "grad_norm": 4.587003231048584,
      "learning_rate": 4.094777562862669e-06,
      "loss": 0.6543,
      "step": 13500
    },
    {
      "epoch": 8.680333119795003,
      "grad_norm": 5.990558624267578,
      "learning_rate": 3.998065764023211e-06,
      "loss": 0.6637,
      "step": 13550
    },
    {
      "epoch": 8.712363869314542,
      "grad_norm": 5.296764850616455,
      "learning_rate": 3.901353965183753e-06,
      "loss": 0.6732,
      "step": 13600
    },
    {
      "epoch": 8.74439461883408,
      "grad_norm": 4.160756587982178,
      "learning_rate": 3.804642166344294e-06,
      "loss": 0.6518,
      "step": 13650
    },
    {
      "epoch": 8.77642536835362,
      "grad_norm": 5.366446495056152,
      "learning_rate": 3.7079303675048357e-06,
      "loss": 0.6471,
      "step": 13700
    },
    {
      "epoch": 8.808456117873158,
      "grad_norm": 3.9408085346221924,
      "learning_rate": 3.6112185686653773e-06,
      "loss": 0.6041,
      "step": 13750
    },
    {
      "epoch": 8.840486867392697,
      "grad_norm": 4.9535064697265625,
      "learning_rate": 3.514506769825919e-06,
      "loss": 0.643,
      "step": 13800
    },
    {
      "epoch": 8.872517616912235,
      "grad_norm": 5.324071407318115,
      "learning_rate": 3.417794970986461e-06,
      "loss": 0.6028,
      "step": 13850
    },
    {
      "epoch": 8.904548366431774,
      "grad_norm": 5.254710674285889,
      "learning_rate": 3.321083172147002e-06,
      "loss": 0.6491,
      "step": 13900
    },
    {
      "epoch": 8.936579115951313,
      "grad_norm": 6.040075778961182,
      "learning_rate": 3.224371373307544e-06,
      "loss": 0.6288,
      "step": 13950
    },
    {
      "epoch": 8.968609865470851,
      "grad_norm": 4.7447919845581055,
      "learning_rate": 3.127659574468085e-06,
      "loss": 0.6002,
      "step": 14000
    },
    {
      "epoch": 9.0,
      "eval_loss": 1.3091392517089844,
      "eval_runtime": 2.3521,
      "eval_samples_per_second": 295.06,
      "eval_steps_per_second": 73.978,
      "step": 14049
    },
    {
      "epoch": 9.00064061499039,
      "grad_norm": 4.693140983581543,
      "learning_rate": 3.030947775628627e-06,
      "loss": 0.6995,
      "step": 14050
    },
    {
      "epoch": 9.03267136450993,
      "grad_norm": 4.343562126159668,
      "learning_rate": 2.934235976789168e-06,
      "loss": 0.5811,
      "step": 14100
    },
    {
      "epoch": 9.064702114029469,
      "grad_norm": 5.002589225769043,
      "learning_rate": 2.83752417794971e-06,
      "loss": 0.593,
      "step": 14150
    },
    {
      "epoch": 9.096732863549008,
      "grad_norm": 5.684055328369141,
      "learning_rate": 2.7408123791102513e-06,
      "loss": 0.6371,
      "step": 14200
    },
    {
      "epoch": 9.128763613068546,
      "grad_norm": 6.824464321136475,
      "learning_rate": 2.6441005802707933e-06,
      "loss": 0.5834,
      "step": 14250
    },
    {
      "epoch": 9.160794362588085,
      "grad_norm": 5.53258752822876,
      "learning_rate": 2.5473887814313344e-06,
      "loss": 0.6127,
      "step": 14300
    },
    {
      "epoch": 9.192825112107624,
      "grad_norm": 5.860435485839844,
      "learning_rate": 2.4506769825918764e-06,
      "loss": 0.6163,
      "step": 14350
    },
    {
      "epoch": 9.224855861627162,
      "grad_norm": 4.696398735046387,
      "learning_rate": 2.353965183752418e-06,
      "loss": 0.5922,
      "step": 14400
    },
    {
      "epoch": 9.256886611146701,
      "grad_norm": 4.38474702835083,
      "learning_rate": 2.2572533849129595e-06,
      "loss": 0.5752,
      "step": 14450
    },
    {
      "epoch": 9.28891736066624,
      "grad_norm": 5.909097194671631,
      "learning_rate": 2.160541586073501e-06,
      "loss": 0.6386,
      "step": 14500
    },
    {
      "epoch": 9.320948110185778,
      "grad_norm": 5.496297359466553,
      "learning_rate": 2.0638297872340426e-06,
      "loss": 0.6116,
      "step": 14550
    },
    {
      "epoch": 9.352978859705317,
      "grad_norm": 5.162509918212891,
      "learning_rate": 1.967117988394584e-06,
      "loss": 0.5986,
      "step": 14600
    },
    {
      "epoch": 9.385009609224856,
      "grad_norm": 3.838149309158325,
      "learning_rate": 1.8704061895551257e-06,
      "loss": 0.6208,
      "step": 14650
    },
    {
      "epoch": 9.417040358744394,
      "grad_norm": 6.156980037689209,
      "learning_rate": 1.7736943907156675e-06,
      "loss": 0.6162,
      "step": 14700
    },
    {
      "epoch": 9.449071108263933,
      "grad_norm": 5.597775459289551,
      "learning_rate": 1.676982591876209e-06,
      "loss": 0.6121,
      "step": 14750
    },
    {
      "epoch": 9.481101857783472,
      "grad_norm": 4.518224239349365,
      "learning_rate": 1.5802707930367506e-06,
      "loss": 0.6122,
      "step": 14800
    },
    {
      "epoch": 9.51313260730301,
      "grad_norm": 4.053903579711914,
      "learning_rate": 1.4835589941972922e-06,
      "loss": 0.6316,
      "step": 14850
    },
    {
      "epoch": 9.545163356822549,
      "grad_norm": 4.227862358093262,
      "learning_rate": 1.3868471953578337e-06,
      "loss": 0.5871,
      "step": 14900
    },
    {
      "epoch": 9.577194106342088,
      "grad_norm": 5.427918910980225,
      "learning_rate": 1.2901353965183753e-06,
      "loss": 0.6238,
      "step": 14950
    },
    {
      "epoch": 9.609224855861626,
      "grad_norm": 5.440342903137207,
      "learning_rate": 1.195357833655706e-06,
      "loss": 0.6427,
      "step": 15000
    },
    {
      "epoch": 9.641255605381167,
      "grad_norm": 4.275557994842529,
      "learning_rate": 1.0986460348162476e-06,
      "loss": 0.5709,
      "step": 15050
    },
    {
      "epoch": 9.673286354900705,
      "grad_norm": 6.184557914733887,
      "learning_rate": 1.0019342359767892e-06,
      "loss": 0.6376,
      "step": 15100
    },
    {
      "epoch": 9.705317104420244,
      "grad_norm": 4.230554103851318,
      "learning_rate": 9.052224371373308e-07,
      "loss": 0.6033,
      "step": 15150
    },
    {
      "epoch": 9.737347853939783,
      "grad_norm": 5.858420372009277,
      "learning_rate": 8.085106382978724e-07,
      "loss": 0.6145,
      "step": 15200
    },
    {
      "epoch": 9.769378603459321,
      "grad_norm": 7.0790886878967285,
      "learning_rate": 7.13733075435203e-07,
      "loss": 0.6304,
      "step": 15250
    },
    {
      "epoch": 9.80140935297886,
      "grad_norm": 6.2126030921936035,
      "learning_rate": 6.170212765957446e-07,
      "loss": 0.6081,
      "step": 15300
    },
    {
      "epoch": 9.833440102498399,
      "grad_norm": 6.42111349105835,
      "learning_rate": 5.203094777562864e-07,
      "loss": 0.6201,
      "step": 15350
    },
    {
      "epoch": 9.865470852017937,
      "grad_norm": 3.662987470626831,
      "learning_rate": 4.2359767891682783e-07,
      "loss": 0.6002,
      "step": 15400
    },
    {
      "epoch": 9.897501601537476,
      "grad_norm": 5.193423748016357,
      "learning_rate": 3.2688588007736944e-07,
      "loss": 0.6013,
      "step": 15450
    },
    {
      "epoch": 9.929532351057015,
      "grad_norm": 5.657595157623291,
      "learning_rate": 2.3017408123791102e-07,
      "loss": 0.6087,
      "step": 15500
    },
    {
      "epoch": 9.961563100576553,
      "grad_norm": 4.873049259185791,
      "learning_rate": 1.334622823984526e-07,
      "loss": 0.6195,
      "step": 15550
    },
    {
      "epoch": 9.993593850096092,
      "grad_norm": 4.241012096405029,
      "learning_rate": 3.67504835589942e-08,
      "loss": 0.6015,
      "step": 15600
    },
    {
      "epoch": 10.0,
      "eval_loss": 1.3214519023895264,
      "eval_runtime": 2.3037,
      "eval_samples_per_second": 301.257,
      "eval_steps_per_second": 75.531,
      "step": 15610
    }
  ],
  "logging_steps": 50,
  "max_steps": 15610,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4757468715417600.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
