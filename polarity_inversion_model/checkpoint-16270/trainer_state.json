{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 16270,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.03073140749846343,
      "grad_norm": 11.615219116210938,
      "learning_rate": 1.4099999999999999e-05,
      "loss": 2.7809,
      "step": 50
    },
    {
      "epoch": 0.06146281499692686,
      "grad_norm": 8.195572853088379,
      "learning_rate": 2.91e-05,
      "loss": 2.0501,
      "step": 100
    },
    {
      "epoch": 0.09219422249539029,
      "grad_norm": 8.171292304992676,
      "learning_rate": 2.991280148423006e-05,
      "loss": 1.8674,
      "step": 150
    },
    {
      "epoch": 0.12292562999385372,
      "grad_norm": 6.840893745422363,
      "learning_rate": 2.9820037105751393e-05,
      "loss": 1.7499,
      "step": 200
    },
    {
      "epoch": 0.15365703749231716,
      "grad_norm": 6.9245924949646,
      "learning_rate": 2.972727272727273e-05,
      "loss": 1.6526,
      "step": 250
    },
    {
      "epoch": 0.18438844499078058,
      "grad_norm": 7.6575493812561035,
      "learning_rate": 2.9634508348794064e-05,
      "loss": 1.6726,
      "step": 300
    },
    {
      "epoch": 0.215119852489244,
      "grad_norm": 6.9001054763793945,
      "learning_rate": 2.9541743970315397e-05,
      "loss": 1.6755,
      "step": 350
    },
    {
      "epoch": 0.24585125998770743,
      "grad_norm": 9.884418487548828,
      "learning_rate": 2.9448979591836735e-05,
      "loss": 1.6279,
      "step": 400
    },
    {
      "epoch": 0.2765826674861709,
      "grad_norm": 6.422761917114258,
      "learning_rate": 2.9356215213358068e-05,
      "loss": 1.6095,
      "step": 450
    },
    {
      "epoch": 0.3073140749846343,
      "grad_norm": 6.205721855163574,
      "learning_rate": 2.926345083487941e-05,
      "loss": 1.6444,
      "step": 500
    },
    {
      "epoch": 0.33804548248309774,
      "grad_norm": 7.011818885803223,
      "learning_rate": 2.9170686456400742e-05,
      "loss": 1.5335,
      "step": 550
    },
    {
      "epoch": 0.36877688998156116,
      "grad_norm": 5.844351768493652,
      "learning_rate": 2.907792207792208e-05,
      "loss": 1.5394,
      "step": 600
    },
    {
      "epoch": 0.3995082974800246,
      "grad_norm": 6.489218711853027,
      "learning_rate": 2.8985157699443413e-05,
      "loss": 1.6021,
      "step": 650
    },
    {
      "epoch": 0.430239704978488,
      "grad_norm": 5.96297550201416,
      "learning_rate": 2.889239332096475e-05,
      "loss": 1.5866,
      "step": 700
    },
    {
      "epoch": 0.46097111247695144,
      "grad_norm": 6.542981147766113,
      "learning_rate": 2.8799628942486084e-05,
      "loss": 1.4517,
      "step": 750
    },
    {
      "epoch": 0.49170251997541486,
      "grad_norm": 10.244729995727539,
      "learning_rate": 2.870686456400742e-05,
      "loss": 1.5599,
      "step": 800
    },
    {
      "epoch": 0.5224339274738783,
      "grad_norm": 8.010496139526367,
      "learning_rate": 2.8614100185528758e-05,
      "loss": 1.4909,
      "step": 850
    },
    {
      "epoch": 0.5531653349723418,
      "grad_norm": 7.918569564819336,
      "learning_rate": 2.8521335807050095e-05,
      "loss": 1.574,
      "step": 900
    },
    {
      "epoch": 0.5838967424708051,
      "grad_norm": 8.340553283691406,
      "learning_rate": 2.842857142857143e-05,
      "loss": 1.5256,
      "step": 950
    },
    {
      "epoch": 0.6146281499692686,
      "grad_norm": 7.360565185546875,
      "learning_rate": 2.8337662337662336e-05,
      "loss": 1.5253,
      "step": 1000
    },
    {
      "epoch": 0.645359557467732,
      "grad_norm": 5.696600437164307,
      "learning_rate": 2.8244897959183673e-05,
      "loss": 1.4601,
      "step": 1050
    },
    {
      "epoch": 0.6760909649661955,
      "grad_norm": 8.046162605285645,
      "learning_rate": 2.8152133580705007e-05,
      "loss": 1.4681,
      "step": 1100
    },
    {
      "epoch": 0.7068223724646588,
      "grad_norm": 9.427770614624023,
      "learning_rate": 2.8059369202226344e-05,
      "loss": 1.5279,
      "step": 1150
    },
    {
      "epoch": 0.7375537799631223,
      "grad_norm": 5.57356595993042,
      "learning_rate": 2.796660482374768e-05,
      "loss": 1.481,
      "step": 1200
    },
    {
      "epoch": 0.7682851874615857,
      "grad_norm": 5.955831050872803,
      "learning_rate": 2.787384044526902e-05,
      "loss": 1.5445,
      "step": 1250
    },
    {
      "epoch": 0.7990165949600492,
      "grad_norm": 6.073838233947754,
      "learning_rate": 2.7781076066790352e-05,
      "loss": 1.5873,
      "step": 1300
    },
    {
      "epoch": 0.8297480024585125,
      "grad_norm": 7.869855880737305,
      "learning_rate": 2.768831168831169e-05,
      "loss": 1.454,
      "step": 1350
    },
    {
      "epoch": 0.860479409956976,
      "grad_norm": 5.210875034332275,
      "learning_rate": 2.7595547309833023e-05,
      "loss": 1.4138,
      "step": 1400
    },
    {
      "epoch": 0.8912108174554395,
      "grad_norm": 5.534940719604492,
      "learning_rate": 2.750278293135436e-05,
      "loss": 1.4117,
      "step": 1450
    },
    {
      "epoch": 0.9219422249539029,
      "grad_norm": 6.453709602355957,
      "learning_rate": 2.7410018552875694e-05,
      "loss": 1.4147,
      "step": 1500
    },
    {
      "epoch": 0.9526736324523664,
      "grad_norm": 8.314969062805176,
      "learning_rate": 2.7317254174397034e-05,
      "loss": 1.4896,
      "step": 1550
    },
    {
      "epoch": 0.9834050399508297,
      "grad_norm": 4.738516330718994,
      "learning_rate": 2.7224489795918368e-05,
      "loss": 1.4491,
      "step": 1600
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.2732186317443848,
      "eval_runtime": 2.553,
      "eval_samples_per_second": 283.585,
      "eval_steps_per_second": 70.896,
      "step": 1627
    },
    {
      "epoch": 1.014136447449293,
      "grad_norm": 6.794857501983643,
      "learning_rate": 2.7131725417439705e-05,
      "loss": 1.4132,
      "step": 1650
    },
    {
      "epoch": 1.0448678549477566,
      "grad_norm": 4.573091506958008,
      "learning_rate": 2.703896103896104e-05,
      "loss": 1.2214,
      "step": 1700
    },
    {
      "epoch": 1.07559926244622,
      "grad_norm": 5.140799045562744,
      "learning_rate": 2.6946196660482376e-05,
      "loss": 1.3419,
      "step": 1750
    },
    {
      "epoch": 1.1063306699446835,
      "grad_norm": 5.951702117919922,
      "learning_rate": 2.685343228200371e-05,
      "loss": 1.2118,
      "step": 1800
    },
    {
      "epoch": 1.1370620774431468,
      "grad_norm": 7.991751194000244,
      "learning_rate": 2.6760667903525047e-05,
      "loss": 1.2615,
      "step": 1850
    },
    {
      "epoch": 1.1677934849416103,
      "grad_norm": 5.421994209289551,
      "learning_rate": 2.6667903525046384e-05,
      "loss": 1.295,
      "step": 1900
    },
    {
      "epoch": 1.1985248924400738,
      "grad_norm": 5.780598163604736,
      "learning_rate": 2.657513914656772e-05,
      "loss": 1.3594,
      "step": 1950
    },
    {
      "epoch": 1.2292562999385372,
      "grad_norm": 4.890115737915039,
      "learning_rate": 2.6482374768089054e-05,
      "loss": 1.2993,
      "step": 2000
    },
    {
      "epoch": 1.2599877074370007,
      "grad_norm": 5.501354694366455,
      "learning_rate": 2.638961038961039e-05,
      "loss": 1.1947,
      "step": 2050
    },
    {
      "epoch": 1.290719114935464,
      "grad_norm": 6.371389389038086,
      "learning_rate": 2.6296846011131725e-05,
      "loss": 1.2733,
      "step": 2100
    },
    {
      "epoch": 1.3214505224339275,
      "grad_norm": 8.058211326599121,
      "learning_rate": 2.6204081632653062e-05,
      "loss": 1.2603,
      "step": 2150
    },
    {
      "epoch": 1.352181929932391,
      "grad_norm": 8.441730499267578,
      "learning_rate": 2.6111317254174396e-05,
      "loss": 1.269,
      "step": 2200
    },
    {
      "epoch": 1.3829133374308542,
      "grad_norm": 4.978793621063232,
      "learning_rate": 2.6018552875695733e-05,
      "loss": 1.3488,
      "step": 2250
    },
    {
      "epoch": 1.4136447449293177,
      "grad_norm": 6.332705974578857,
      "learning_rate": 2.592578849721707e-05,
      "loss": 1.3277,
      "step": 2300
    },
    {
      "epoch": 1.4443761524277812,
      "grad_norm": 4.723349094390869,
      "learning_rate": 2.5833024118738407e-05,
      "loss": 1.2112,
      "step": 2350
    },
    {
      "epoch": 1.4751075599262446,
      "grad_norm": 5.552082061767578,
      "learning_rate": 2.574025974025974e-05,
      "loss": 1.2645,
      "step": 2400
    },
    {
      "epoch": 1.5058389674247081,
      "grad_norm": 8.382494926452637,
      "learning_rate": 2.5647495361781078e-05,
      "loss": 1.1713,
      "step": 2450
    },
    {
      "epoch": 1.5365703749231714,
      "grad_norm": 8.636134147644043,
      "learning_rate": 2.555473098330241e-05,
      "loss": 1.2768,
      "step": 2500
    },
    {
      "epoch": 1.5673017824216349,
      "grad_norm": 10.258561134338379,
      "learning_rate": 2.546196660482375e-05,
      "loss": 1.2685,
      "step": 2550
    },
    {
      "epoch": 1.5980331899200984,
      "grad_norm": 7.039050102233887,
      "learning_rate": 2.5369202226345082e-05,
      "loss": 1.2489,
      "step": 2600
    },
    {
      "epoch": 1.6287645974185616,
      "grad_norm": 5.070517539978027,
      "learning_rate": 2.5276437847866423e-05,
      "loss": 1.2403,
      "step": 2650
    },
    {
      "epoch": 1.6594960049170253,
      "grad_norm": 6.071409225463867,
      "learning_rate": 2.5183673469387757e-05,
      "loss": 1.3219,
      "step": 2700
    },
    {
      "epoch": 1.6902274124154886,
      "grad_norm": 5.79597282409668,
      "learning_rate": 2.509090909090909e-05,
      "loss": 1.3265,
      "step": 2750
    },
    {
      "epoch": 1.720958819913952,
      "grad_norm": 5.118382930755615,
      "learning_rate": 2.4998144712430427e-05,
      "loss": 1.2744,
      "step": 2800
    },
    {
      "epoch": 1.7516902274124155,
      "grad_norm": 6.523242950439453,
      "learning_rate": 2.490538033395176e-05,
      "loss": 1.2613,
      "step": 2850
    },
    {
      "epoch": 1.7824216349108788,
      "grad_norm": 4.6089768409729,
      "learning_rate": 2.4812615955473098e-05,
      "loss": 1.2509,
      "step": 2900
    },
    {
      "epoch": 1.8131530424093425,
      "grad_norm": 5.071998596191406,
      "learning_rate": 2.4719851576994432e-05,
      "loss": 1.266,
      "step": 2950
    },
    {
      "epoch": 1.8438844499078058,
      "grad_norm": 4.188558578491211,
      "learning_rate": 2.4627087198515772e-05,
      "loss": 1.2738,
      "step": 3000
    },
    {
      "epoch": 1.8746158574062692,
      "grad_norm": 4.084261417388916,
      "learning_rate": 2.4534322820037106e-05,
      "loss": 1.2261,
      "step": 3050
    },
    {
      "epoch": 1.9053472649047327,
      "grad_norm": 4.7925262451171875,
      "learning_rate": 2.4441558441558443e-05,
      "loss": 1.198,
      "step": 3100
    },
    {
      "epoch": 1.936078672403196,
      "grad_norm": 4.639428615570068,
      "learning_rate": 2.4348794063079777e-05,
      "loss": 1.2195,
      "step": 3150
    },
    {
      "epoch": 1.9668100799016595,
      "grad_norm": 7.443498611450195,
      "learning_rate": 2.4256029684601114e-05,
      "loss": 1.2164,
      "step": 3200
    },
    {
      "epoch": 1.997541487400123,
      "grad_norm": 6.0704522132873535,
      "learning_rate": 2.4163265306122448e-05,
      "loss": 1.2698,
      "step": 3250
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.2114999294281006,
      "eval_runtime": 2.5021,
      "eval_samples_per_second": 289.356,
      "eval_steps_per_second": 72.339,
      "step": 3254
    },
    {
      "epoch": 2.028272894898586,
      "grad_norm": 4.975903034210205,
      "learning_rate": 2.4070500927643785e-05,
      "loss": 1.1621,
      "step": 3300
    },
    {
      "epoch": 2.05900430239705,
      "grad_norm": 5.034065246582031,
      "learning_rate": 2.3977736549165122e-05,
      "loss": 1.0407,
      "step": 3350
    },
    {
      "epoch": 2.089735709895513,
      "grad_norm": 6.230437278747559,
      "learning_rate": 2.388497217068646e-05,
      "loss": 1.0745,
      "step": 3400
    },
    {
      "epoch": 2.120467117393977,
      "grad_norm": 5.830694675445557,
      "learning_rate": 2.3792207792207793e-05,
      "loss": 1.0461,
      "step": 3450
    },
    {
      "epoch": 2.15119852489244,
      "grad_norm": 4.017455577850342,
      "learning_rate": 2.369944341372913e-05,
      "loss": 1.1199,
      "step": 3500
    },
    {
      "epoch": 2.1819299323909034,
      "grad_norm": 4.310374736785889,
      "learning_rate": 2.3606679035250463e-05,
      "loss": 1.0852,
      "step": 3550
    },
    {
      "epoch": 2.212661339889367,
      "grad_norm": 4.689793109893799,
      "learning_rate": 2.35139146567718e-05,
      "loss": 1.1135,
      "step": 3600
    },
    {
      "epoch": 2.2433927473878303,
      "grad_norm": 4.094457626342773,
      "learning_rate": 2.3421150278293134e-05,
      "loss": 1.119,
      "step": 3650
    },
    {
      "epoch": 2.2741241548862936,
      "grad_norm": 4.974456787109375,
      "learning_rate": 2.332838589981447e-05,
      "loss": 1.0361,
      "step": 3700
    },
    {
      "epoch": 2.3048555623847573,
      "grad_norm": 5.954516410827637,
      "learning_rate": 2.3235621521335808e-05,
      "loss": 1.06,
      "step": 3750
    },
    {
      "epoch": 2.3355869698832206,
      "grad_norm": 4.902896881103516,
      "learning_rate": 2.3142857142857145e-05,
      "loss": 1.0861,
      "step": 3800
    },
    {
      "epoch": 2.3663183773816843,
      "grad_norm": 4.864041328430176,
      "learning_rate": 2.305009276437848e-05,
      "loss": 1.1231,
      "step": 3850
    },
    {
      "epoch": 2.3970497848801475,
      "grad_norm": 6.26948881149292,
      "learning_rate": 2.2957328385899816e-05,
      "loss": 1.1383,
      "step": 3900
    },
    {
      "epoch": 2.427781192378611,
      "grad_norm": 4.342827796936035,
      "learning_rate": 2.286456400742115e-05,
      "loss": 1.1148,
      "step": 3950
    },
    {
      "epoch": 2.4585125998770745,
      "grad_norm": 4.631886959075928,
      "learning_rate": 2.2773654916512057e-05,
      "loss": 1.0278,
      "step": 4000
    },
    {
      "epoch": 2.4892440073755377,
      "grad_norm": 5.232298851013184,
      "learning_rate": 2.2680890538033398e-05,
      "loss": 1.1328,
      "step": 4050
    },
    {
      "epoch": 2.5199754148740015,
      "grad_norm": 4.430607795715332,
      "learning_rate": 2.258812615955473e-05,
      "loss": 1.1304,
      "step": 4100
    },
    {
      "epoch": 2.5507068223724647,
      "grad_norm": 4.235769271850586,
      "learning_rate": 2.249536178107607e-05,
      "loss": 1.028,
      "step": 4150
    },
    {
      "epoch": 2.581438229870928,
      "grad_norm": 5.463257789611816,
      "learning_rate": 2.2402597402597402e-05,
      "loss": 1.0985,
      "step": 4200
    },
    {
      "epoch": 2.6121696373693917,
      "grad_norm": 5.163358688354492,
      "learning_rate": 2.230983302411874e-05,
      "loss": 1.13,
      "step": 4250
    },
    {
      "epoch": 2.642901044867855,
      "grad_norm": 6.605260848999023,
      "learning_rate": 2.2217068645640073e-05,
      "loss": 1.1568,
      "step": 4300
    },
    {
      "epoch": 2.673632452366318,
      "grad_norm": 8.210248947143555,
      "learning_rate": 2.212430426716141e-05,
      "loss": 1.0879,
      "step": 4350
    },
    {
      "epoch": 2.704363859864782,
      "grad_norm": 3.543281316757202,
      "learning_rate": 2.2031539888682747e-05,
      "loss": 1.1103,
      "step": 4400
    },
    {
      "epoch": 2.735095267363245,
      "grad_norm": 6.303553581237793,
      "learning_rate": 2.1938775510204084e-05,
      "loss": 1.0681,
      "step": 4450
    },
    {
      "epoch": 2.7658266748617084,
      "grad_norm": 8.303438186645508,
      "learning_rate": 2.1846011131725418e-05,
      "loss": 1.0766,
      "step": 4500
    },
    {
      "epoch": 2.796558082360172,
      "grad_norm": 5.263467311859131,
      "learning_rate": 2.1753246753246755e-05,
      "loss": 1.0211,
      "step": 4550
    },
    {
      "epoch": 2.8272894898586354,
      "grad_norm": 5.45673942565918,
      "learning_rate": 2.166048237476809e-05,
      "loss": 1.1355,
      "step": 4600
    },
    {
      "epoch": 2.858020897357099,
      "grad_norm": 5.675716876983643,
      "learning_rate": 2.1567717996289426e-05,
      "loss": 1.0836,
      "step": 4650
    },
    {
      "epoch": 2.8887523048555623,
      "grad_norm": 8.799732208251953,
      "learning_rate": 2.147495361781076e-05,
      "loss": 1.0998,
      "step": 4700
    },
    {
      "epoch": 2.919483712354026,
      "grad_norm": 5.326507568359375,
      "learning_rate": 2.1382189239332097e-05,
      "loss": 1.0873,
      "step": 4750
    },
    {
      "epoch": 2.9502151198524893,
      "grad_norm": 5.710530757904053,
      "learning_rate": 2.1289424860853434e-05,
      "loss": 1.1033,
      "step": 4800
    },
    {
      "epoch": 2.9809465273509526,
      "grad_norm": 4.405362606048584,
      "learning_rate": 2.119666048237477e-05,
      "loss": 1.1627,
      "step": 4850
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.2067826986312866,
      "eval_runtime": 2.5186,
      "eval_samples_per_second": 287.464,
      "eval_steps_per_second": 71.866,
      "step": 4881
    },
    {
      "epoch": 3.0116779348494163,
      "grad_norm": 4.966373920440674,
      "learning_rate": 2.1103896103896105e-05,
      "loss": 1.0269,
      "step": 4900
    },
    {
      "epoch": 3.0424093423478795,
      "grad_norm": 6.160760402679443,
      "learning_rate": 2.101113172541744e-05,
      "loss": 0.9612,
      "step": 4950
    },
    {
      "epoch": 3.073140749846343,
      "grad_norm": 4.509362697601318,
      "learning_rate": 2.0918367346938775e-05,
      "loss": 0.9172,
      "step": 5000
    },
    {
      "epoch": 3.1038721573448065,
      "grad_norm": 6.082810878753662,
      "learning_rate": 2.0825602968460112e-05,
      "loss": 0.982,
      "step": 5050
    },
    {
      "epoch": 3.1346035648432697,
      "grad_norm": 6.152050018310547,
      "learning_rate": 2.0732838589981446e-05,
      "loss": 0.9392,
      "step": 5100
    },
    {
      "epoch": 3.1653349723417334,
      "grad_norm": 4.451178073883057,
      "learning_rate": 2.0640074211502783e-05,
      "loss": 0.9597,
      "step": 5150
    },
    {
      "epoch": 3.1960663798401967,
      "grad_norm": 4.7636260986328125,
      "learning_rate": 2.054730983302412e-05,
      "loss": 0.8795,
      "step": 5200
    },
    {
      "epoch": 3.22679778733866,
      "grad_norm": 5.152946949005127,
      "learning_rate": 2.0454545454545454e-05,
      "loss": 0.9345,
      "step": 5250
    },
    {
      "epoch": 3.2575291948371237,
      "grad_norm": 4.171358108520508,
      "learning_rate": 2.036178107606679e-05,
      "loss": 0.9439,
      "step": 5300
    },
    {
      "epoch": 3.288260602335587,
      "grad_norm": 5.131450176239014,
      "learning_rate": 2.0269016697588125e-05,
      "loss": 1.0363,
      "step": 5350
    },
    {
      "epoch": 3.3189920098340506,
      "grad_norm": 9.143906593322754,
      "learning_rate": 2.0176252319109462e-05,
      "loss": 0.9409,
      "step": 5400
    },
    {
      "epoch": 3.349723417332514,
      "grad_norm": 5.4638447761535645,
      "learning_rate": 2.0083487940630796e-05,
      "loss": 0.8865,
      "step": 5450
    },
    {
      "epoch": 3.380454824830977,
      "grad_norm": 5.968716621398926,
      "learning_rate": 1.9990723562152136e-05,
      "loss": 0.9338,
      "step": 5500
    },
    {
      "epoch": 3.411186232329441,
      "grad_norm": 4.650004863739014,
      "learning_rate": 1.989795918367347e-05,
      "loss": 0.8883,
      "step": 5550
    },
    {
      "epoch": 3.441917639827904,
      "grad_norm": 5.747798442840576,
      "learning_rate": 1.9805194805194807e-05,
      "loss": 0.9887,
      "step": 5600
    },
    {
      "epoch": 3.4726490473263674,
      "grad_norm": 6.0018534660339355,
      "learning_rate": 1.971243042671614e-05,
      "loss": 0.9195,
      "step": 5650
    },
    {
      "epoch": 3.503380454824831,
      "grad_norm": 5.281290531158447,
      "learning_rate": 1.9619666048237478e-05,
      "loss": 1.0277,
      "step": 5700
    },
    {
      "epoch": 3.5341118623232943,
      "grad_norm": 9.502103805541992,
      "learning_rate": 1.952690166975881e-05,
      "loss": 0.9656,
      "step": 5750
    },
    {
      "epoch": 3.5648432698217576,
      "grad_norm": 4.876263618469238,
      "learning_rate": 1.943413729128015e-05,
      "loss": 0.9545,
      "step": 5800
    },
    {
      "epoch": 3.5955746773202213,
      "grad_norm": 7.214212894439697,
      "learning_rate": 1.9341372912801485e-05,
      "loss": 1.0187,
      "step": 5850
    },
    {
      "epoch": 3.6263060848186845,
      "grad_norm": 5.915482521057129,
      "learning_rate": 1.9248608534322823e-05,
      "loss": 0.9843,
      "step": 5900
    },
    {
      "epoch": 3.6570374923171483,
      "grad_norm": 5.765779495239258,
      "learning_rate": 1.9155844155844156e-05,
      "loss": 0.9734,
      "step": 5950
    },
    {
      "epoch": 3.6877688998156115,
      "grad_norm": 5.481149196624756,
      "learning_rate": 1.9063079777365493e-05,
      "loss": 0.9706,
      "step": 6000
    },
    {
      "epoch": 3.718500307314075,
      "grad_norm": 5.117895126342773,
      "learning_rate": 1.8970315398886827e-05,
      "loss": 0.9385,
      "step": 6050
    },
    {
      "epoch": 3.7492317148125385,
      "grad_norm": 5.8244242668151855,
      "learning_rate": 1.8877551020408164e-05,
      "loss": 1.0255,
      "step": 6100
    },
    {
      "epoch": 3.7799631223110017,
      "grad_norm": 6.2468438148498535,
      "learning_rate": 1.8784786641929498e-05,
      "loss": 0.9804,
      "step": 6150
    },
    {
      "epoch": 3.8106945298094654,
      "grad_norm": 5.513153553009033,
      "learning_rate": 1.8692022263450835e-05,
      "loss": 1.0135,
      "step": 6200
    },
    {
      "epoch": 3.8414259373079287,
      "grad_norm": 5.216897964477539,
      "learning_rate": 1.8599257884972172e-05,
      "loss": 1.0,
      "step": 6250
    },
    {
      "epoch": 3.872157344806392,
      "grad_norm": 5.384924411773682,
      "learning_rate": 1.850649350649351e-05,
      "loss": 0.9244,
      "step": 6300
    },
    {
      "epoch": 3.9028887523048557,
      "grad_norm": 5.2002105712890625,
      "learning_rate": 1.8413729128014843e-05,
      "loss": 0.9664,
      "step": 6350
    },
    {
      "epoch": 3.933620159803319,
      "grad_norm": 5.469965934753418,
      "learning_rate": 1.832096474953618e-05,
      "loss": 0.9499,
      "step": 6400
    },
    {
      "epoch": 3.964351567301782,
      "grad_norm": 4.626544952392578,
      "learning_rate": 1.8228200371057514e-05,
      "loss": 0.9895,
      "step": 6450
    },
    {
      "epoch": 3.995082974800246,
      "grad_norm": 3.681521415710449,
      "learning_rate": 1.813543599257885e-05,
      "loss": 1.0122,
      "step": 6500
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.2116026878356934,
      "eval_runtime": 2.5258,
      "eval_samples_per_second": 286.645,
      "eval_steps_per_second": 71.661,
      "step": 6508
    },
    {
      "epoch": 4.02581438229871,
      "grad_norm": 5.979097366333008,
      "learning_rate": 1.8042671614100184e-05,
      "loss": 0.9099,
      "step": 6550
    },
    {
      "epoch": 4.056545789797172,
      "grad_norm": 7.027499675750732,
      "learning_rate": 1.7949907235621525e-05,
      "loss": 0.9574,
      "step": 6600
    },
    {
      "epoch": 4.087277197295636,
      "grad_norm": 6.052661895751953,
      "learning_rate": 1.785714285714286e-05,
      "loss": 0.8415,
      "step": 6650
    },
    {
      "epoch": 4.1180086047941,
      "grad_norm": 5.2338151931762695,
      "learning_rate": 1.7764378478664196e-05,
      "loss": 0.9089,
      "step": 6700
    },
    {
      "epoch": 4.148740012292563,
      "grad_norm": 5.332066059112549,
      "learning_rate": 1.767161410018553e-05,
      "loss": 0.8155,
      "step": 6750
    },
    {
      "epoch": 4.179471419791026,
      "grad_norm": 5.146416187286377,
      "learning_rate": 1.7578849721706866e-05,
      "loss": 0.8516,
      "step": 6800
    },
    {
      "epoch": 4.21020282728949,
      "grad_norm": 5.533736705780029,
      "learning_rate": 1.74860853432282e-05,
      "loss": 0.8449,
      "step": 6850
    },
    {
      "epoch": 4.240934234787954,
      "grad_norm": 5.092403888702393,
      "learning_rate": 1.7393320964749534e-05,
      "loss": 0.7925,
      "step": 6900
    },
    {
      "epoch": 4.2716656422864165,
      "grad_norm": 4.333446979522705,
      "learning_rate": 1.7300556586270874e-05,
      "loss": 0.8249,
      "step": 6950
    },
    {
      "epoch": 4.30239704978488,
      "grad_norm": 5.115786552429199,
      "learning_rate": 1.7207792207792208e-05,
      "loss": 0.9109,
      "step": 7000
    },
    {
      "epoch": 4.333128457283344,
      "grad_norm": 7.103363037109375,
      "learning_rate": 1.7115027829313545e-05,
      "loss": 0.8612,
      "step": 7050
    },
    {
      "epoch": 4.363859864781807,
      "grad_norm": 5.455411911010742,
      "learning_rate": 1.702226345083488e-05,
      "loss": 0.8875,
      "step": 7100
    },
    {
      "epoch": 4.3945912722802705,
      "grad_norm": 5.289163589477539,
      "learning_rate": 1.6929499072356216e-05,
      "loss": 0.8515,
      "step": 7150
    },
    {
      "epoch": 4.425322679778734,
      "grad_norm": 3.7006499767303467,
      "learning_rate": 1.683673469387755e-05,
      "loss": 0.8366,
      "step": 7200
    },
    {
      "epoch": 4.456054087277197,
      "grad_norm": 4.461051940917969,
      "learning_rate": 1.6743970315398887e-05,
      "loss": 0.8306,
      "step": 7250
    },
    {
      "epoch": 4.486785494775661,
      "grad_norm": 6.118541240692139,
      "learning_rate": 1.665120593692022e-05,
      "loss": 0.8983,
      "step": 7300
    },
    {
      "epoch": 4.517516902274124,
      "grad_norm": 5.990533828735352,
      "learning_rate": 1.655844155844156e-05,
      "loss": 0.9081,
      "step": 7350
    },
    {
      "epoch": 4.548248309772587,
      "grad_norm": 6.084194183349609,
      "learning_rate": 1.6465677179962894e-05,
      "loss": 0.8464,
      "step": 7400
    },
    {
      "epoch": 4.578979717271051,
      "grad_norm": 5.374527454376221,
      "learning_rate": 1.637291280148423e-05,
      "loss": 0.8108,
      "step": 7450
    },
    {
      "epoch": 4.609711124769515,
      "grad_norm": 4.064593315124512,
      "learning_rate": 1.6280148423005565e-05,
      "loss": 0.8374,
      "step": 7500
    },
    {
      "epoch": 4.640442532267977,
      "grad_norm": 3.9163427352905273,
      "learning_rate": 1.6187384044526902e-05,
      "loss": 0.8856,
      "step": 7550
    },
    {
      "epoch": 4.671173939766441,
      "grad_norm": 8.47920036315918,
      "learning_rate": 1.6094619666048236e-05,
      "loss": 0.9377,
      "step": 7600
    },
    {
      "epoch": 4.701905347264905,
      "grad_norm": 5.478064060211182,
      "learning_rate": 1.6001855287569573e-05,
      "loss": 0.852,
      "step": 7650
    },
    {
      "epoch": 4.7326367547633685,
      "grad_norm": 4.839848041534424,
      "learning_rate": 1.590909090909091e-05,
      "loss": 0.916,
      "step": 7700
    },
    {
      "epoch": 4.763368162261831,
      "grad_norm": 5.982937335968018,
      "learning_rate": 1.5816326530612247e-05,
      "loss": 0.843,
      "step": 7750
    },
    {
      "epoch": 4.794099569760295,
      "grad_norm": 4.796107769012451,
      "learning_rate": 1.572356215213358e-05,
      "loss": 0.8709,
      "step": 7800
    },
    {
      "epoch": 4.824830977258759,
      "grad_norm": 5.508295059204102,
      "learning_rate": 1.5630797773654918e-05,
      "loss": 0.8523,
      "step": 7850
    },
    {
      "epoch": 4.855562384757222,
      "grad_norm": 5.150069713592529,
      "learning_rate": 1.5538033395176252e-05,
      "loss": 0.9187,
      "step": 7900
    },
    {
      "epoch": 4.886293792255685,
      "grad_norm": 6.068060398101807,
      "learning_rate": 1.544526901669759e-05,
      "loss": 0.8667,
      "step": 7950
    },
    {
      "epoch": 4.917025199754149,
      "grad_norm": 9.155350685119629,
      "learning_rate": 1.5352504638218923e-05,
      "loss": 0.9089,
      "step": 8000
    },
    {
      "epoch": 4.947756607252612,
      "grad_norm": 5.468790054321289,
      "learning_rate": 1.5259740259740263e-05,
      "loss": 0.8377,
      "step": 8050
    },
    {
      "epoch": 4.9784880147510755,
      "grad_norm": 4.284388065338135,
      "learning_rate": 1.5166975881261595e-05,
      "loss": 0.9068,
      "step": 8100
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.221666932106018,
      "eval_runtime": 2.5168,
      "eval_samples_per_second": 287.669,
      "eval_steps_per_second": 71.917,
      "step": 8135
    },
    {
      "epoch": 5.009219422249539,
      "grad_norm": 5.712055206298828,
      "learning_rate": 1.5074211502782934e-05,
      "loss": 0.8142,
      "step": 8150
    },
    {
      "epoch": 5.039950829748003,
      "grad_norm": 4.088535785675049,
      "learning_rate": 1.4981447124304267e-05,
      "loss": 0.7918,
      "step": 8200
    },
    {
      "epoch": 5.070682237246466,
      "grad_norm": 6.175699234008789,
      "learning_rate": 1.4888682745825603e-05,
      "loss": 0.785,
      "step": 8250
    },
    {
      "epoch": 5.101413644744929,
      "grad_norm": 3.855654239654541,
      "learning_rate": 1.4795918367346938e-05,
      "loss": 0.7539,
      "step": 8300
    },
    {
      "epoch": 5.132145052243393,
      "grad_norm": 4.640708923339844,
      "learning_rate": 1.4703153988868275e-05,
      "loss": 0.8055,
      "step": 8350
    },
    {
      "epoch": 5.162876459741856,
      "grad_norm": 6.308600902557373,
      "learning_rate": 1.461038961038961e-05,
      "loss": 0.74,
      "step": 8400
    },
    {
      "epoch": 5.19360786724032,
      "grad_norm": 5.488581657409668,
      "learning_rate": 1.4517625231910946e-05,
      "loss": 0.8436,
      "step": 8450
    },
    {
      "epoch": 5.224339274738783,
      "grad_norm": 4.576596736907959,
      "learning_rate": 1.4424860853432281e-05,
      "loss": 0.7358,
      "step": 8500
    },
    {
      "epoch": 5.255070682237246,
      "grad_norm": 5.057453155517578,
      "learning_rate": 1.4332096474953619e-05,
      "loss": 0.7919,
      "step": 8550
    },
    {
      "epoch": 5.28580208973571,
      "grad_norm": 3.9984447956085205,
      "learning_rate": 1.4239332096474954e-05,
      "loss": 0.7378,
      "step": 8600
    },
    {
      "epoch": 5.316533497234174,
      "grad_norm": 6.173676490783691,
      "learning_rate": 1.414656771799629e-05,
      "loss": 0.7938,
      "step": 8650
    },
    {
      "epoch": 5.347264904732636,
      "grad_norm": 5.591365337371826,
      "learning_rate": 1.4053803339517626e-05,
      "loss": 0.7217,
      "step": 8700
    },
    {
      "epoch": 5.3779963122311,
      "grad_norm": 4.600505828857422,
      "learning_rate": 1.3961038961038962e-05,
      "loss": 0.7362,
      "step": 8750
    },
    {
      "epoch": 5.408727719729564,
      "grad_norm": 5.7982001304626465,
      "learning_rate": 1.3868274582560297e-05,
      "loss": 0.821,
      "step": 8800
    },
    {
      "epoch": 5.439459127228027,
      "grad_norm": 5.168809413909912,
      "learning_rate": 1.3775510204081633e-05,
      "loss": 0.7803,
      "step": 8850
    },
    {
      "epoch": 5.47019053472649,
      "grad_norm": 4.396413803100586,
      "learning_rate": 1.368274582560297e-05,
      "loss": 0.7978,
      "step": 8900
    },
    {
      "epoch": 5.500921942224954,
      "grad_norm": 5.0808634757995605,
      "learning_rate": 1.3589981447124305e-05,
      "loss": 0.8269,
      "step": 8950
    },
    {
      "epoch": 5.531653349723418,
      "grad_norm": 4.258156776428223,
      "learning_rate": 1.349721706864564e-05,
      "loss": 0.7968,
      "step": 9000
    },
    {
      "epoch": 5.5623847572218805,
      "grad_norm": 4.949164390563965,
      "learning_rate": 1.3404452690166976e-05,
      "loss": 0.8228,
      "step": 9050
    },
    {
      "epoch": 5.593116164720344,
      "grad_norm": 6.1845831871032715,
      "learning_rate": 1.3311688311688313e-05,
      "loss": 0.7876,
      "step": 9100
    },
    {
      "epoch": 5.623847572218808,
      "grad_norm": 5.294000625610352,
      "learning_rate": 1.3218923933209648e-05,
      "loss": 0.793,
      "step": 9150
    },
    {
      "epoch": 5.654578979717271,
      "grad_norm": 3.3792002201080322,
      "learning_rate": 1.3126159554730984e-05,
      "loss": 0.7932,
      "step": 9200
    },
    {
      "epoch": 5.6853103872157345,
      "grad_norm": 5.380223274230957,
      "learning_rate": 1.303339517625232e-05,
      "loss": 0.755,
      "step": 9250
    },
    {
      "epoch": 5.716041794714198,
      "grad_norm": 6.528711318969727,
      "learning_rate": 1.2940630797773656e-05,
      "loss": 0.7704,
      "step": 9300
    },
    {
      "epoch": 5.746773202212661,
      "grad_norm": 4.635975360870361,
      "learning_rate": 1.2847866419294992e-05,
      "loss": 0.8265,
      "step": 9350
    },
    {
      "epoch": 5.777504609711125,
      "grad_norm": 5.122495651245117,
      "learning_rate": 1.2755102040816325e-05,
      "loss": 0.6974,
      "step": 9400
    },
    {
      "epoch": 5.808236017209588,
      "grad_norm": 5.392216205596924,
      "learning_rate": 1.2662337662337662e-05,
      "loss": 0.7417,
      "step": 9450
    },
    {
      "epoch": 5.838967424708052,
      "grad_norm": 4.257260322570801,
      "learning_rate": 1.2569573283858998e-05,
      "loss": 0.821,
      "step": 9500
    },
    {
      "epoch": 5.869698832206515,
      "grad_norm": 4.8820390701293945,
      "learning_rate": 1.2476808905380333e-05,
      "loss": 0.8291,
      "step": 9550
    },
    {
      "epoch": 5.900430239704979,
      "grad_norm": 5.585369110107422,
      "learning_rate": 1.238404452690167e-05,
      "loss": 0.7333,
      "step": 9600
    },
    {
      "epoch": 5.931161647203442,
      "grad_norm": 6.046340465545654,
      "learning_rate": 1.2291280148423006e-05,
      "loss": 0.7846,
      "step": 9650
    },
    {
      "epoch": 5.961893054701905,
      "grad_norm": 4.86208438873291,
      "learning_rate": 1.2198515769944341e-05,
      "loss": 0.8238,
      "step": 9700
    },
    {
      "epoch": 5.992624462200369,
      "grad_norm": 4.630396366119385,
      "learning_rate": 1.2105751391465676e-05,
      "loss": 0.8311,
      "step": 9750
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.2286611795425415,
      "eval_runtime": 2.5132,
      "eval_samples_per_second": 288.073,
      "eval_steps_per_second": 72.018,
      "step": 9762
    },
    {
      "epoch": 6.0233558696988325,
      "grad_norm": 6.0495500564575195,
      "learning_rate": 1.2012987012987014e-05,
      "loss": 0.7533,
      "step": 9800
    },
    {
      "epoch": 6.054087277197295,
      "grad_norm": 5.924208641052246,
      "learning_rate": 1.1920222634508349e-05,
      "loss": 0.7499,
      "step": 9850
    },
    {
      "epoch": 6.084818684695759,
      "grad_norm": 4.458508014678955,
      "learning_rate": 1.1827458256029684e-05,
      "loss": 0.7141,
      "step": 9900
    },
    {
      "epoch": 6.115550092194223,
      "grad_norm": 4.852433681488037,
      "learning_rate": 1.173469387755102e-05,
      "loss": 0.7062,
      "step": 9950
    },
    {
      "epoch": 6.146281499692686,
      "grad_norm": 6.080296039581299,
      "learning_rate": 1.1641929499072357e-05,
      "loss": 0.7382,
      "step": 10000
    },
    {
      "epoch": 6.177012907191149,
      "grad_norm": 4.737798690795898,
      "learning_rate": 1.1549165120593692e-05,
      "loss": 0.6687,
      "step": 10050
    },
    {
      "epoch": 6.207744314689613,
      "grad_norm": 4.7755818367004395,
      "learning_rate": 1.1456400742115028e-05,
      "loss": 0.6722,
      "step": 10100
    },
    {
      "epoch": 6.238475722188076,
      "grad_norm": 7.466798305511475,
      "learning_rate": 1.1363636363636365e-05,
      "loss": 0.7213,
      "step": 10150
    },
    {
      "epoch": 6.2692071296865395,
      "grad_norm": 5.721119403839111,
      "learning_rate": 1.12708719851577e-05,
      "loss": 0.7743,
      "step": 10200
    },
    {
      "epoch": 6.299938537185003,
      "grad_norm": 5.222634315490723,
      "learning_rate": 1.1178107606679035e-05,
      "loss": 0.7486,
      "step": 10250
    },
    {
      "epoch": 6.330669944683467,
      "grad_norm": 6.031162261962891,
      "learning_rate": 1.108534322820037e-05,
      "loss": 0.7227,
      "step": 10300
    },
    {
      "epoch": 6.36140135218193,
      "grad_norm": 6.614114761352539,
      "learning_rate": 1.0992578849721708e-05,
      "loss": 0.734,
      "step": 10350
    },
    {
      "epoch": 6.392132759680393,
      "grad_norm": 5.742454528808594,
      "learning_rate": 1.0899814471243043e-05,
      "loss": 0.6737,
      "step": 10400
    },
    {
      "epoch": 6.422864167178857,
      "grad_norm": 4.60693883895874,
      "learning_rate": 1.0807050092764379e-05,
      "loss": 0.7338,
      "step": 10450
    },
    {
      "epoch": 6.45359557467732,
      "grad_norm": 7.384335517883301,
      "learning_rate": 1.0714285714285714e-05,
      "loss": 0.7192,
      "step": 10500
    },
    {
      "epoch": 6.484326982175784,
      "grad_norm": 6.384156703948975,
      "learning_rate": 1.0621521335807051e-05,
      "loss": 0.6833,
      "step": 10550
    },
    {
      "epoch": 6.515058389674247,
      "grad_norm": 4.068350315093994,
      "learning_rate": 1.0528756957328387e-05,
      "loss": 0.7305,
      "step": 10600
    },
    {
      "epoch": 6.54578979717271,
      "grad_norm": 6.228622913360596,
      "learning_rate": 1.0435992578849722e-05,
      "loss": 0.715,
      "step": 10650
    },
    {
      "epoch": 6.576521204671174,
      "grad_norm": 5.997067451477051,
      "learning_rate": 1.0343228200371059e-05,
      "loss": 0.7611,
      "step": 10700
    },
    {
      "epoch": 6.6072526121696376,
      "grad_norm": 5.4619855880737305,
      "learning_rate": 1.0250463821892394e-05,
      "loss": 0.7436,
      "step": 10750
    },
    {
      "epoch": 6.637984019668101,
      "grad_norm": 7.876067161560059,
      "learning_rate": 1.015769944341373e-05,
      "loss": 0.7141,
      "step": 10800
    },
    {
      "epoch": 6.668715427166564,
      "grad_norm": 5.613064289093018,
      "learning_rate": 1.0064935064935065e-05,
      "loss": 0.7051,
      "step": 10850
    },
    {
      "epoch": 6.699446834665028,
      "grad_norm": 5.672866344451904,
      "learning_rate": 9.972170686456402e-06,
      "loss": 0.6862,
      "step": 10900
    },
    {
      "epoch": 6.7301782421634915,
      "grad_norm": 5.0871381759643555,
      "learning_rate": 9.879406307977738e-06,
      "loss": 0.7161,
      "step": 10950
    },
    {
      "epoch": 6.760909649661954,
      "grad_norm": 4.468475818634033,
      "learning_rate": 9.786641929499073e-06,
      "loss": 0.7756,
      "step": 11000
    },
    {
      "epoch": 6.791641057160418,
      "grad_norm": 5.818033695220947,
      "learning_rate": 9.693877551020408e-06,
      "loss": 0.6993,
      "step": 11050
    },
    {
      "epoch": 6.822372464658882,
      "grad_norm": 4.6216535568237305,
      "learning_rate": 9.601113172541746e-06,
      "loss": 0.7415,
      "step": 11100
    },
    {
      "epoch": 6.8531038721573445,
      "grad_norm": 7.512811183929443,
      "learning_rate": 9.50834879406308e-06,
      "loss": 0.747,
      "step": 11150
    },
    {
      "epoch": 6.883835279655808,
      "grad_norm": 5.606546401977539,
      "learning_rate": 9.415584415584415e-06,
      "loss": 0.7759,
      "step": 11200
    },
    {
      "epoch": 6.914566687154272,
      "grad_norm": 7.2589545249938965,
      "learning_rate": 9.322820037105752e-06,
      "loss": 0.7063,
      "step": 11250
    },
    {
      "epoch": 6.945298094652735,
      "grad_norm": 4.423707485198975,
      "learning_rate": 9.230055658627087e-06,
      "loss": 0.7369,
      "step": 11300
    },
    {
      "epoch": 6.976029502151198,
      "grad_norm": 4.993800163269043,
      "learning_rate": 9.137291280148422e-06,
      "loss": 0.744,
      "step": 11350
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.2407673597335815,
      "eval_runtime": 2.5141,
      "eval_samples_per_second": 287.971,
      "eval_steps_per_second": 71.993,
      "step": 11389
    },
    {
      "epoch": 7.006760909649662,
      "grad_norm": 5.056506633758545,
      "learning_rate": 9.044526901669758e-06,
      "loss": 0.6935,
      "step": 11400
    },
    {
      "epoch": 7.037492317148125,
      "grad_norm": 6.222658634185791,
      "learning_rate": 8.951762523191095e-06,
      "loss": 0.6289,
      "step": 11450
    },
    {
      "epoch": 7.068223724646589,
      "grad_norm": 4.143347263336182,
      "learning_rate": 8.85899814471243e-06,
      "loss": 0.6528,
      "step": 11500
    },
    {
      "epoch": 7.098955132145052,
      "grad_norm": 4.835549831390381,
      "learning_rate": 8.766233766233766e-06,
      "loss": 0.7173,
      "step": 11550
    },
    {
      "epoch": 7.129686539643516,
      "grad_norm": 5.052938938140869,
      "learning_rate": 8.673469387755101e-06,
      "loss": 0.6644,
      "step": 11600
    },
    {
      "epoch": 7.160417947141979,
      "grad_norm": 3.9148471355438232,
      "learning_rate": 8.580705009276438e-06,
      "loss": 0.6317,
      "step": 11650
    },
    {
      "epoch": 7.191149354640443,
      "grad_norm": 5.213162899017334,
      "learning_rate": 8.487940630797774e-06,
      "loss": 0.611,
      "step": 11700
    },
    {
      "epoch": 7.221880762138906,
      "grad_norm": 6.393876075744629,
      "learning_rate": 8.395176252319109e-06,
      "loss": 0.6705,
      "step": 11750
    },
    {
      "epoch": 7.252612169637369,
      "grad_norm": 6.580448150634766,
      "learning_rate": 8.302411873840446e-06,
      "loss": 0.6337,
      "step": 11800
    },
    {
      "epoch": 7.283343577135833,
      "grad_norm": 4.937806606292725,
      "learning_rate": 8.209647495361781e-06,
      "loss": 0.6691,
      "step": 11850
    },
    {
      "epoch": 7.3140749846342965,
      "grad_norm": 5.098639965057373,
      "learning_rate": 8.116883116883117e-06,
      "loss": 0.6693,
      "step": 11900
    },
    {
      "epoch": 7.344806392132759,
      "grad_norm": 3.481826066970825,
      "learning_rate": 8.024118738404452e-06,
      "loss": 0.7126,
      "step": 11950
    },
    {
      "epoch": 7.375537799631223,
      "grad_norm": 5.399585247039795,
      "learning_rate": 7.93135435992579e-06,
      "loss": 0.6851,
      "step": 12000
    },
    {
      "epoch": 7.406269207129687,
      "grad_norm": 4.436145305633545,
      "learning_rate": 7.840445269016697e-06,
      "loss": 0.6655,
      "step": 12050
    },
    {
      "epoch": 7.43700061462815,
      "grad_norm": 4.675042152404785,
      "learning_rate": 7.747680890538032e-06,
      "loss": 0.7029,
      "step": 12100
    },
    {
      "epoch": 7.467732022126613,
      "grad_norm": 4.151533126831055,
      "learning_rate": 7.65491651205937e-06,
      "loss": 0.6553,
      "step": 12150
    },
    {
      "epoch": 7.498463429625077,
      "grad_norm": 4.8491716384887695,
      "learning_rate": 7.562152133580705e-06,
      "loss": 0.6204,
      "step": 12200
    },
    {
      "epoch": 7.529194837123541,
      "grad_norm": 5.450109004974365,
      "learning_rate": 7.469387755102041e-06,
      "loss": 0.6598,
      "step": 12250
    },
    {
      "epoch": 7.5599262446220035,
      "grad_norm": 4.402227401733398,
      "learning_rate": 7.376623376623376e-06,
      "loss": 0.6554,
      "step": 12300
    },
    {
      "epoch": 7.590657652120467,
      "grad_norm": 7.477687358856201,
      "learning_rate": 7.283858998144713e-06,
      "loss": 0.6842,
      "step": 12350
    },
    {
      "epoch": 7.621389059618931,
      "grad_norm": 6.338891983032227,
      "learning_rate": 7.191094619666048e-06,
      "loss": 0.6644,
      "step": 12400
    },
    {
      "epoch": 7.652120467117394,
      "grad_norm": 5.466665744781494,
      "learning_rate": 7.098330241187384e-06,
      "loss": 0.7508,
      "step": 12450
    },
    {
      "epoch": 7.682851874615857,
      "grad_norm": 4.686848163604736,
      "learning_rate": 7.00556586270872e-06,
      "loss": 0.6614,
      "step": 12500
    },
    {
      "epoch": 7.713583282114321,
      "grad_norm": 5.129816055297852,
      "learning_rate": 6.912801484230056e-06,
      "loss": 0.671,
      "step": 12550
    },
    {
      "epoch": 7.744314689612784,
      "grad_norm": 5.5803751945495605,
      "learning_rate": 6.820037105751391e-06,
      "loss": 0.6958,
      "step": 12600
    },
    {
      "epoch": 7.775046097111248,
      "grad_norm": 6.622759819030762,
      "learning_rate": 6.7272727272727275e-06,
      "loss": 0.6754,
      "step": 12650
    },
    {
      "epoch": 7.805777504609711,
      "grad_norm": 4.601315498352051,
      "learning_rate": 6.634508348794064e-06,
      "loss": 0.6726,
      "step": 12700
    },
    {
      "epoch": 7.836508912108174,
      "grad_norm": 2.9322926998138428,
      "learning_rate": 6.541743970315399e-06,
      "loss": 0.6559,
      "step": 12750
    },
    {
      "epoch": 7.867240319606638,
      "grad_norm": 4.638143539428711,
      "learning_rate": 6.448979591836735e-06,
      "loss": 0.6754,
      "step": 12800
    },
    {
      "epoch": 7.8979717271051015,
      "grad_norm": 5.298969745635986,
      "learning_rate": 6.356215213358071e-06,
      "loss": 0.6792,
      "step": 12850
    },
    {
      "epoch": 7.928703134603564,
      "grad_norm": 4.560816287994385,
      "learning_rate": 6.263450834879407e-06,
      "loss": 0.5846,
      "step": 12900
    },
    {
      "epoch": 7.959434542102028,
      "grad_norm": 5.677850246429443,
      "learning_rate": 6.170686456400742e-06,
      "loss": 0.7,
      "step": 12950
    },
    {
      "epoch": 7.990165949600492,
      "grad_norm": 5.4256439208984375,
      "learning_rate": 6.077922077922079e-06,
      "loss": 0.6817,
      "step": 13000
    },
    {
      "epoch": 8.0,
      "eval_loss": 1.260547399520874,
      "eval_runtime": 2.5229,
      "eval_samples_per_second": 286.974,
      "eval_steps_per_second": 71.743,
      "step": 13016
    },
    {
      "epoch": 8.020897357098955,
      "grad_norm": 4.906295299530029,
      "learning_rate": 5.985157699443414e-06,
      "loss": 0.6019,
      "step": 13050
    },
    {
      "epoch": 8.05162876459742,
      "grad_norm": 8.606898307800293,
      "learning_rate": 5.892393320964749e-06,
      "loss": 0.6513,
      "step": 13100
    },
    {
      "epoch": 8.082360172095882,
      "grad_norm": 5.613394260406494,
      "learning_rate": 5.799628942486085e-06,
      "loss": 0.6325,
      "step": 13150
    },
    {
      "epoch": 8.113091579594345,
      "grad_norm": 6.077009201049805,
      "learning_rate": 5.706864564007421e-06,
      "loss": 0.6595,
      "step": 13200
    },
    {
      "epoch": 8.14382298709281,
      "grad_norm": 4.341039657592773,
      "learning_rate": 5.614100185528757e-06,
      "loss": 0.6024,
      "step": 13250
    },
    {
      "epoch": 8.174554394591272,
      "grad_norm": 4.173720359802246,
      "learning_rate": 5.521335807050093e-06,
      "loss": 0.6199,
      "step": 13300
    },
    {
      "epoch": 8.205285802089735,
      "grad_norm": 3.3980650901794434,
      "learning_rate": 5.428571428571429e-06,
      "loss": 0.667,
      "step": 13350
    },
    {
      "epoch": 8.2360172095882,
      "grad_norm": 4.509989261627197,
      "learning_rate": 5.335807050092764e-06,
      "loss": 0.6613,
      "step": 13400
    },
    {
      "epoch": 8.266748617086662,
      "grad_norm": 5.035059452056885,
      "learning_rate": 5.2430426716141005e-06,
      "loss": 0.676,
      "step": 13450
    },
    {
      "epoch": 8.297480024585125,
      "grad_norm": 4.686277389526367,
      "learning_rate": 5.150278293135436e-06,
      "loss": 0.6563,
      "step": 13500
    },
    {
      "epoch": 8.32821143208359,
      "grad_norm": 5.907585144042969,
      "learning_rate": 5.057513914656772e-06,
      "loss": 0.6709,
      "step": 13550
    },
    {
      "epoch": 8.358942839582053,
      "grad_norm": 5.3694233894348145,
      "learning_rate": 4.9647495361781075e-06,
      "loss": 0.6524,
      "step": 13600
    },
    {
      "epoch": 8.389674247080515,
      "grad_norm": 4.933590888977051,
      "learning_rate": 4.871985157699444e-06,
      "loss": 0.646,
      "step": 13650
    },
    {
      "epoch": 8.42040565457898,
      "grad_norm": 5.065260887145996,
      "learning_rate": 4.779220779220779e-06,
      "loss": 0.6511,
      "step": 13700
    },
    {
      "epoch": 8.451137062077443,
      "grad_norm": 5.63603401184082,
      "learning_rate": 4.686456400742115e-06,
      "loss": 0.5494,
      "step": 13750
    },
    {
      "epoch": 8.481868469575907,
      "grad_norm": 4.361560344696045,
      "learning_rate": 4.593692022263452e-06,
      "loss": 0.6308,
      "step": 13800
    },
    {
      "epoch": 8.51259987707437,
      "grad_norm": 5.1691107749938965,
      "learning_rate": 4.500927643784787e-06,
      "loss": 0.6269,
      "step": 13850
    },
    {
      "epoch": 8.543331284572833,
      "grad_norm": 5.629035472869873,
      "learning_rate": 4.408163265306123e-06,
      "loss": 0.5974,
      "step": 13900
    },
    {
      "epoch": 8.574062692071298,
      "grad_norm": 7.016727447509766,
      "learning_rate": 4.315398886827458e-06,
      "loss": 0.6073,
      "step": 13950
    },
    {
      "epoch": 8.60479409956976,
      "grad_norm": 5.4588541984558105,
      "learning_rate": 4.222634508348794e-06,
      "loss": 0.5963,
      "step": 14000
    },
    {
      "epoch": 8.635525507068223,
      "grad_norm": 4.897109031677246,
      "learning_rate": 4.1298701298701294e-06,
      "loss": 0.6,
      "step": 14050
    },
    {
      "epoch": 8.666256914566688,
      "grad_norm": 4.79812479019165,
      "learning_rate": 4.037105751391466e-06,
      "loss": 0.6134,
      "step": 14100
    },
    {
      "epoch": 8.69698832206515,
      "grad_norm": 5.747678756713867,
      "learning_rate": 3.944341372912801e-06,
      "loss": 0.6224,
      "step": 14150
    },
    {
      "epoch": 8.727719729563614,
      "grad_norm": 5.031090259552002,
      "learning_rate": 3.853432282003711e-06,
      "loss": 0.5932,
      "step": 14200
    },
    {
      "epoch": 8.758451137062078,
      "grad_norm": 7.549820423126221,
      "learning_rate": 3.7606679035250465e-06,
      "loss": 0.6357,
      "step": 14250
    },
    {
      "epoch": 8.789182544560541,
      "grad_norm": 4.934276580810547,
      "learning_rate": 3.6697588126159557e-06,
      "loss": 0.6243,
      "step": 14300
    },
    {
      "epoch": 8.819913952059004,
      "grad_norm": 4.636702060699463,
      "learning_rate": 3.5769944341372915e-06,
      "loss": 0.6861,
      "step": 14350
    },
    {
      "epoch": 8.850645359557468,
      "grad_norm": 4.411250114440918,
      "learning_rate": 3.4842300556586273e-06,
      "loss": 0.6044,
      "step": 14400
    },
    {
      "epoch": 8.881376767055931,
      "grad_norm": 8.0920991897583,
      "learning_rate": 3.391465677179963e-06,
      "loss": 0.6519,
      "step": 14450
    },
    {
      "epoch": 8.912108174554394,
      "grad_norm": 6.293148040771484,
      "learning_rate": 3.298701298701299e-06,
      "loss": 0.6003,
      "step": 14500
    },
    {
      "epoch": 8.942839582052859,
      "grad_norm": 3.47755765914917,
      "learning_rate": 3.2059369202226347e-06,
      "loss": 0.6182,
      "step": 14550
    },
    {
      "epoch": 8.973570989551321,
      "grad_norm": 5.558206558227539,
      "learning_rate": 3.11317254174397e-06,
      "loss": 0.6049,
      "step": 14600
    },
    {
      "epoch": 9.0,
      "eval_loss": 1.2751212120056152,
      "eval_runtime": 2.5062,
      "eval_samples_per_second": 288.889,
      "eval_steps_per_second": 72.222,
      "step": 14643
    },
    {
      "epoch": 9.004302397049784,
      "grad_norm": 4.972342491149902,
      "learning_rate": 3.0222634508348793e-06,
      "loss": 0.6267,
      "step": 14650
    },
    {
      "epoch": 9.035033804548249,
      "grad_norm": 5.05445671081543,
      "learning_rate": 2.9294990723562156e-06,
      "loss": 0.604,
      "step": 14700
    },
    {
      "epoch": 9.065765212046712,
      "grad_norm": 4.13041353225708,
      "learning_rate": 2.8367346938775514e-06,
      "loss": 0.5938,
      "step": 14750
    },
    {
      "epoch": 9.096496619545174,
      "grad_norm": 5.616197109222412,
      "learning_rate": 2.743970315398887e-06,
      "loss": 0.5991,
      "step": 14800
    },
    {
      "epoch": 9.127228027043639,
      "grad_norm": 4.926839828491211,
      "learning_rate": 2.6512059369202226e-06,
      "loss": 0.6101,
      "step": 14850
    },
    {
      "epoch": 9.157959434542102,
      "grad_norm": 6.152791500091553,
      "learning_rate": 2.5584415584415584e-06,
      "loss": 0.5921,
      "step": 14900
    },
    {
      "epoch": 9.188690842040565,
      "grad_norm": 4.715744495391846,
      "learning_rate": 2.4656771799628942e-06,
      "loss": 0.6205,
      "step": 14950
    },
    {
      "epoch": 9.21942224953903,
      "grad_norm": 4.068018436431885,
      "learning_rate": 2.37291280148423e-06,
      "loss": 0.5833,
      "step": 15000
    },
    {
      "epoch": 9.250153657037492,
      "grad_norm": 2.8739304542541504,
      "learning_rate": 2.280148423005566e-06,
      "loss": 0.5629,
      "step": 15050
    },
    {
      "epoch": 9.280885064535955,
      "grad_norm": 4.58984899520874,
      "learning_rate": 2.1873840445269017e-06,
      "loss": 0.601,
      "step": 15100
    },
    {
      "epoch": 9.31161647203442,
      "grad_norm": 5.247560977935791,
      "learning_rate": 2.0946196660482375e-06,
      "loss": 0.5926,
      "step": 15150
    },
    {
      "epoch": 9.342347879532882,
      "grad_norm": 5.055922508239746,
      "learning_rate": 2.0018552875695737e-06,
      "loss": 0.6129,
      "step": 15200
    },
    {
      "epoch": 9.373079287031347,
      "grad_norm": 5.149010181427002,
      "learning_rate": 1.909090909090909e-06,
      "loss": 0.61,
      "step": 15250
    },
    {
      "epoch": 9.40381069452981,
      "grad_norm": 5.056394100189209,
      "learning_rate": 1.816326530612245e-06,
      "loss": 0.6166,
      "step": 15300
    },
    {
      "epoch": 9.434542102028272,
      "grad_norm": 4.749068737030029,
      "learning_rate": 1.7235621521335807e-06,
      "loss": 0.5985,
      "step": 15350
    },
    {
      "epoch": 9.465273509526737,
      "grad_norm": 4.074717998504639,
      "learning_rate": 1.6307977736549165e-06,
      "loss": 0.5963,
      "step": 15400
    },
    {
      "epoch": 9.4960049170252,
      "grad_norm": 4.208230018615723,
      "learning_rate": 1.5380333951762524e-06,
      "loss": 0.6144,
      "step": 15450
    },
    {
      "epoch": 9.526736324523663,
      "grad_norm": 3.816561222076416,
      "learning_rate": 1.4452690166975882e-06,
      "loss": 0.5947,
      "step": 15500
    },
    {
      "epoch": 9.557467732022127,
      "grad_norm": 5.977999687194824,
      "learning_rate": 1.352504638218924e-06,
      "loss": 0.6378,
      "step": 15550
    },
    {
      "epoch": 9.58819913952059,
      "grad_norm": 3.8486745357513428,
      "learning_rate": 1.2597402597402598e-06,
      "loss": 0.5942,
      "step": 15600
    },
    {
      "epoch": 9.618930547019053,
      "grad_norm": 5.609817028045654,
      "learning_rate": 1.1669758812615956e-06,
      "loss": 0.619,
      "step": 15650
    },
    {
      "epoch": 9.649661954517518,
      "grad_norm": 8.024988174438477,
      "learning_rate": 1.0742115027829314e-06,
      "loss": 0.604,
      "step": 15700
    },
    {
      "epoch": 9.68039336201598,
      "grad_norm": 5.572363376617432,
      "learning_rate": 9.814471243042672e-07,
      "loss": 0.6093,
      "step": 15750
    },
    {
      "epoch": 9.711124769514443,
      "grad_norm": 5.655058860778809,
      "learning_rate": 8.886827458256031e-07,
      "loss": 0.5938,
      "step": 15800
    },
    {
      "epoch": 9.741856177012908,
      "grad_norm": 4.858628749847412,
      "learning_rate": 7.959183673469388e-07,
      "loss": 0.6019,
      "step": 15850
    },
    {
      "epoch": 9.77258758451137,
      "grad_norm": 4.380483150482178,
      "learning_rate": 7.031539888682746e-07,
      "loss": 0.628,
      "step": 15900
    },
    {
      "epoch": 9.803318992009833,
      "grad_norm": 4.522301197052002,
      "learning_rate": 6.103896103896104e-07,
      "loss": 0.6047,
      "step": 15950
    },
    {
      "epoch": 9.834050399508298,
      "grad_norm": 4.903183460235596,
      "learning_rate": 5.176252319109462e-07,
      "loss": 0.542,
      "step": 16000
    },
    {
      "epoch": 9.86478180700676,
      "grad_norm": 4.590135097503662,
      "learning_rate": 4.24860853432282e-07,
      "loss": 0.6651,
      "step": 16050
    },
    {
      "epoch": 9.895513214505224,
      "grad_norm": 9.10566520690918,
      "learning_rate": 3.3209647495361784e-07,
      "loss": 0.569,
      "step": 16100
    },
    {
      "epoch": 9.926244622003688,
      "grad_norm": 3.7469115257263184,
      "learning_rate": 2.393320964749536e-07,
      "loss": 0.5734,
      "step": 16150
    },
    {
      "epoch": 9.956976029502151,
      "grad_norm": 6.199322700500488,
      "learning_rate": 1.4656771799628942e-07,
      "loss": 0.6095,
      "step": 16200
    },
    {
      "epoch": 9.987707437000614,
      "grad_norm": 5.392483711242676,
      "learning_rate": 5.380333951762523e-08,
      "loss": 0.6058,
      "step": 16250
    },
    {
      "epoch": 10.0,
      "eval_loss": 1.2823090553283691,
      "eval_runtime": 2.4658,
      "eval_samples_per_second": 293.614,
      "eval_steps_per_second": 73.404,
      "step": 16270
    }
  ],
  "logging_steps": 50,
  "max_steps": 16270,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4959443917209600.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
