{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "This block imports the necessary libraries for text preprocessing, machine learning, and deep learning. It also verifies GPU availability for TensorFlow operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries for data manipulation, machine learning, and deep learning\n",
    "import pandas as pd  # For data handling\n",
    "import numpy as np  # For numerical operations\n",
    "import re  # For regular expressions\n",
    "import string  # For string manipulation\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import StratifiedKFold  # For cross-validation\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix  # Evaluation metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS  # Text feature extraction\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin  # For creating custom classifiers\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic Regression model\n",
    "\n",
    "# Deep learning libraries\n",
    "import tensorflow as tf  # Core deep learning library\n",
    "from tensorflow.keras.models import Sequential  # Sequential model class\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional  # Model layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # Text tokenization\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  # Sequence padding\n",
    "from tensorflow.keras.callbacks import EarlyStopping  # Early stopping during training\n",
    "\n",
    "# For saving models and data\n",
    "import joblib  # For saving and loading scikit-learn models\n",
    "\n",
    "# Check and print available GPUs for TensorFlow\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing\n",
    "This block loads training and test datasets, combines title and text fields, maps class labels to sentiment, and ensures clean data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_df = pd.read_csv('train.csv', header=None)  # Load training data\n",
    "test_df = pd.read_csv('test.csv', header=None)  # Load testing data\n",
    "\n",
    "# Assign column names\n",
    "train_df.columns = ['class', 'title', 'text']  # Assign columns for training data\n",
    "test_df.columns = ['class', 'title', 'text']  # Assign columns for test data\n",
    "\n",
    "# Combine title and text into a single column\n",
    "train_df['review'] = train_df['title'] + ' ' + train_df['text']  # Combine columns for training\n",
    "test_df['review'] = test_df['title'] + ' ' + test_df['text']  # Combine columns for testing\n",
    "\n",
    "# Drop the original title and text columns\n",
    "train_df = train_df.drop(columns=['title', 'text'])  # Remove redundant columns\n",
    "test_df = test_df.drop(columns=['title', 'text'])\n",
    "\n",
    "# Map class labels to binary sentiment\n",
    "def map_class(x):\n",
    "    if x == 1:\n",
    "        return 0  # Negative sentiment\n",
    "    elif x == 2:\n",
    "        return 1  # Positive sentiment\n",
    "    else:\n",
    "        return np.nan  # Invalid class\n",
    "\n",
    "train_df['label'] = train_df['class'].apply(map_class)  # Map class labels for training data\n",
    "test_df['label'] = test_df['class'].apply(map_class)  # Map class labels for test data\n",
    "\n",
    "# Drop rows with NaN labels\n",
    "train_df = train_df.dropna(subset=['label'])\n",
    "test_df = test_df.dropna(subset=['label'])\n",
    "\n",
    "# Drop the original 'class' column\n",
    "train_df = train_df.drop(columns=['class'])  # Remove original class column in training\n",
    "test_df = test_df.drop(columns=['class'])  # Remove original class column in testing\n",
    "\n",
    "# Convert label to integer type\n",
    "train_df['label'] = train_df['label'].astype(int)  # Ensure labels are integers\n",
    "test_df['label'] = test_df['label'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning\n",
    "Defines a function to clean text by removing punctuation, numbers, and stopwords. Applies this function to both training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ENGLISH_STOP_WORDS from scikit-learn\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "stop_words = ENGLISH_STOP_WORDS  # Set of predefined stopwords\n",
    "\n",
    "# Define a function to clean text without NLTK\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()  # Convert text to lowercase\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    tokens = text.split()  # Tokenize text\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    tokens = [word for word in tokens if len(word) > 1]  # Remove short words\n",
    "    cleaned_text = ' '.join(tokens)  # Join tokens into a string\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the cleaning function to the review column\n",
    "train_df['cleaned_review'] = train_df['review'].apply(clean_text)  # Clean training data\n",
    "test_df['cleaned_review'] = test_df['review'].apply(clean_text)  # Clean testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Cleaned Data\n",
    "Saves the cleaned datasets for future reuse, ensuring data consistency across experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to CSV files\n",
    "train_df.to_csv('train_cleaned.csv', index=False)  # Save cleaned training data\n",
    "test_df.to_csv('test_cleaned.csv', index=False)  # Save cleaned test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf Logistic Regression Classifier\n",
    "Defines a custom classifier that uses TF-IDF for text vectorization and Logistic Regression for classification. Includes methods for saving and loading the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfLogisticRegressionClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()  # Initialize TF-IDF vectorizer\n",
    "        self.model = LogisticRegression(max_iter=1000, solver='saga', n_jobs=-1)  # Initialize Logistic Regression\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X_tfidf = self.vectorizer.fit_transform(X)  # Vectorize training data\n",
    "        self.model.fit(X_tfidf, y)  # Train model\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X_tfidf = self.vectorizer.transform(X)  # Vectorize input data\n",
    "        return self.model.predict(X_tfidf)  # Predict labels\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        X_tfidf = self.vectorizer.transform(X)  # Vectorize input data\n",
    "        return self.model.predict_proba(X_tfidf)  # Predict probabilities\n",
    "        \n",
    "    def save(self, model_path, vectorizer_path):\n",
    "        joblib.dump(self.model, model_path)  # Save model\n",
    "        joblib.dump(self.vectorizer, vectorizer_path)  # Save vectorizer\n",
    "        \n",
    "    def load(self, model_path, vectorizer_path):\n",
    "        self.model = joblib.load(model_path)  # Load model\n",
    "        self.vectorizer = joblib.load(vectorizer_path)  # Load vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training Data\n",
    "Prepares the cleaned training data for modeling by separating the features (cleaned reviews) and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned data (if starting from saved cleaned data)\n",
    "# train_df = pd.read_csv('train_cleaned.csv')\n",
    "# test_df = pd.read_csv('test_cleaned.csv')\n",
    "\n",
    "X_train = train_df['cleaned_review']  # Features: cleaned reviews\n",
    "y_train = train_df['label']  # Labels: sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation with Stratified K-Folds\n",
    "Performs cross-validation using stratified folds to evaluate the model's performance and ensure balanced class distribution in each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1...\n",
      "Validation accuracy for fold 1: 0.8989666666666667\n",
      "Training fold 2...\n",
      "Validation accuracy for fold 2: 0.8989486111111111\n",
      "Training fold 3...\n",
      "Validation accuracy for fold 3: 0.8985555555555556\n",
      "Training fold 4...\n",
      "Validation accuracy for fold 4: 0.8987208333333333\n",
      "Training fold 5...\n",
      "Validation accuracy for fold 5: 0.8986763888888889\n",
      "Cross-validation accuracies: [0.8989666666666667, 0.8989486111111111, 0.8985555555555556, 0.8987208333333333, 0.8986763888888889]\n",
      "Mean accuracy: 0.8987736111111111\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation using Stratified K-Folds\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold cross-validation\n",
    "\n",
    "cv_scores = []  # Store cross-validation scores\n",
    "fold_no = 1  # Fold counter\n",
    "\n",
    "for train_index, val_index in kfold.split(X_train, y_train):\n",
    "    print(f\"Training fold {fold_no}...\")\n",
    "    X_tr, X_val = X_train.iloc[train_index], X_train.iloc[val_index]  # Split training and validation sets\n",
    "    y_tr, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    classifier = TfidfLogisticRegressionClassifier()  # Instantiate classifier\n",
    "    classifier.fit(X_tr, y_tr)  # Train classifier\n",
    "\n",
    "    y_pred = classifier.predict(X_val)  # Predict validation labels\n",
    "    acc = accuracy_score(y_val, y_pred)  # Calculate accuracy\n",
    "    print(f\"Validation accuracy for fold {fold_no}: {acc}\")\n",
    "    cv_scores.append(acc)\n",
    "\n",
    "    # Early stopping criterion\n",
    "    if acc >= 0.95:\n",
    "        print(\"Early stopping criterion met.\")\n",
    "        break\n",
    "\n",
    "    fold_no += 1\n",
    "\n",
    "print(\"Cross-validation accuracies:\", cv_scores)\n",
    "print(\"Mean accuracy:\", np.mean(cv_scores))  # Report mean cross-validation accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Model on Entire Training Data\n",
    "Trains the final Tfidf Logistic Regression model using the entire training dataset for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfLogisticRegressionClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;TfidfLogisticRegressionClassifier<span class=\"sk-estimator-doc-link \">i<span>Not fitted</span></span></label><div class=\"sk-toggleable__content \"><pre>TfidfLogisticRegressionClassifier()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "TfidfLogisticRegressionClassifier()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model on the entire training data\n",
    "classifier = TfidfLogisticRegressionClassifier()  # Instantiate classifier\n",
    "classifier.fit(X_train, y_train)  # Train on entire training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "Evaluates the trained model on the test dataset and reports metrics like classification accuracy, confusion matrix, and detailed classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.90    200000\n",
      "           1       0.90      0.90      0.90    200000\n",
      "\n",
      "    accuracy                           0.90    400000\n",
      "   macro avg       0.90      0.90      0.90    400000\n",
      "weighted avg       0.90      0.90      0.90    400000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[178850  21150]\n",
      " [ 19132 180868]]\n",
      "Accuracy: 0.899295\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data\n",
    "X_test = test_df['cleaned_review']  # Features: cleaned reviews\n",
    "y_test = test_df['label']  # Labels: sentiment\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = classifier.predict(X_test)  # Predict test labels\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))  # Detailed metrics\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)  # Confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)  # Overall accuracy\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model and Vectorizer\n",
    "Saves the trained model and vectorizer for reuse in future predictions or deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and vectorizer for future use\n",
    "classifier.save('tfidf_logistic_model.joblib', 'tfidf_vectorizer.joblib')  # Save both model and vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Classifier Definition\n",
    "Defines a custom deep neural network (DNN) classifier using TensorFlow/Keras. The model includes bidirectional LSTM layers for handling sequential data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNClassifier:\n",
    "    def __init__(self, max_words=20000, max_len=300, embedding_dim=128):\n",
    "        self.max_words = max_words  # Vocabulary size\n",
    "        self.max_len = max_len  # Maximum length of sequences\n",
    "        self.embedding_dim = embedding_dim  # Embedding dimensions\n",
    "        self.tokenizer = Tokenizer(num_words=self.max_words)  # Tokenizer for text\n",
    "        self.model = None  # Placeholder for the DNN model\n",
    "        \n",
    "        # Configure GPU settings\n",
    "        physical_devices = tf.config.list_physical_devices('GPU')\n",
    "        if physical_devices:\n",
    "            try:\n",
    "                # Set memory growth\n",
    "                for gpu in physical_devices:\n",
    "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "                print(len(physical_devices), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "            except RuntimeError as e:\n",
    "                print(e)\n",
    "        else:\n",
    "            print(\"No GPU devices available\")\n",
    "            \n",
    "    def build_model(self):\n",
    "        with tf.device('/GPU:0'):\n",
    "            model = Sequential()\n",
    "            model.add(Embedding(self.max_words, self.embedding_dim, input_length=self.max_len))  # Embedding layer\n",
    "            model.add(Bidirectional(LSTM(64, return_sequences=True)))  # First Bidirectional LSTM layer\n",
    "            model.add(Dropout(0.5))  # Dropout for regularization\n",
    "            model.add(Bidirectional(LSTM(32)))  # Second Bidirectional LSTM layer\n",
    "            model.add(Dropout(0.5))  # Dropout for regularization\n",
    "            model.add(Dense(1, activation='sigmoid'))  # Output layer\n",
    "            model.compile(loss='binary_crossentropy',\n",
    "                          optimizer='adam',\n",
    "                          metrics=['accuracy'])\n",
    "        return model  # Return compiled model\n",
    "        \n",
    "    def fit(self, X, y, epochs=10, batch_size=256):\n",
    "        # Tokenize the text\n",
    "        self.tokenizer.fit_on_texts(X)\n",
    "        sequences = self.tokenizer.texts_to_sequences(X)  # Convert texts to sequences\n",
    "        X_padded = pad_sequences(sequences, maxlen=self.max_len)  # Pad sequences\n",
    "        # Build the model\n",
    "        self.model = self.build_model()\n",
    "        # Define early stopping callback\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "        # Fit the model\n",
    "        self.model.fit(X_padded, y, epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks=[early_stop])\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        sequences = self.tokenizer.texts_to_sequences(X)  # Convert texts to sequences\n",
    "        X_padded = pad_sequences(sequences, maxlen=self.max_len)  # Pad sequences\n",
    "        predictions = (self.model.predict(X_padded) > 0.5).astype(\"int32\")  # Predict classes\n",
    "        return predictions.flatten()\n",
    "        \n",
    "    def save(self, model_path, tokenizer_path):\n",
    "        self.model.save(model_path)  # Save the model\n",
    "        with open(tokenizer_path, 'wb') as handle:\n",
    "            joblib.dump(self.tokenizer, handle)  # Save the tokenizer\n",
    "        \n",
    "    def load(self, model_path, tokenizer_path):\n",
    "        self.model = tf.keras.models.load_model(model_path)  # Load the model\n",
    "        with open(tokenizer_path, 'rb') as handle:\n",
    "            self.tokenizer = joblib.load(handle)  # Load the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data for Training\n",
    "Defines the features (`X_train`) and labels (`y_train`) for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "X_train = train_df['cleaned_review']  # Training feature: cleaned review text\n",
    "y_train = train_df['label']  # Training labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Training Data\n",
    "Samples a subset of the training data to optimize training efficiency, useful for limited computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a subset for efficient training (adjust based on system capacity)\n",
    "sample_size = 200000  # Maximum sample size for training\n",
    "X_train_sample = X_train.sample(sample_size, random_state=42)  # Randomly sample training data\n",
    "y_train_sample = y_train.loc[X_train_sample.index]  # Match sampled features with their labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network Classifier - Training\n",
    "Instantiates the DNN-based classifier and trains it with the sampled dataset using early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 22:24:08.240333: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-27 22:24:08.242667: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-27 22:24:08.244530: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-27 22:24:08.330452: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-27 22:24:08.331355: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-27 22:24:08.332147: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-27 22:24:08.332934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5746 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 22:24:12.785922: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 22:24:14.995038: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-11-27 22:24:15.596636: I external/local_xla/xla/service/service.cc:168] XLA service 0x775819d525a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-11-27 22:24:15.596654: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4060, Compute Capability 8.9\n",
      "2024-11-27 22:24:15.599372: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732767855.647844   15818 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704/704 [==============================] - 42s 56ms/step - loss: 0.3188 - accuracy: 0.8639 - val_loss: 0.2571 - val_accuracy: 0.8931\n",
      "Epoch 2/10\n",
      "704/704 [==============================] - 32s 46ms/step - loss: 0.2243 - accuracy: 0.9116 - val_loss: 0.2582 - val_accuracy: 0.8931\n",
      "Epoch 3/10\n",
      "704/704 [==============================] - 32s 45ms/step - loss: 0.1808 - accuracy: 0.9297 - val_loss: 0.2796 - val_accuracy: 0.8886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.DNNClassifier at 0x775a9a0df7f0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the classifier\n",
    "classifier = DNNClassifier()  # Initialize the DNNClassifier instance\n",
    "\n",
    "# Fit the model with early stopping\n",
    "classifier.fit(X_train_sample, y_train_sample, epochs=10, batch_size=256)  # Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the DNN Model\n",
    "Evaluates the model on the test data, generating predictions, a classification report, confusion matrix, and accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500/12500 [==============================] - 118s 9ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89    200000\n",
      "           1       0.88      0.90      0.89    200000\n",
      "\n",
      "    accuracy                           0.89    400000\n",
      "   macro avg       0.89      0.89      0.89    400000\n",
      "weighted avg       0.89      0.89      0.89    400000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[175782  24218]\n",
      " [ 19407 180593]]\n",
      "Accuracy: 0.8909375\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data\n",
    "X_test = test_df['cleaned_review']  # Features for testing\n",
    "y_test = test_df['label']  # Labels for testing\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = classifier.predict(X_test)  # Generate predictions\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))  # Display detailed classification metrics\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)  # Generate confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)  # Compute accuracy\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the DNN Model\n",
    "Saves the trained DNN model and its tokenizer for future inference tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toru/anaconda3/envs/tf_gpu_2_15/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model and tokenizer for future use\n",
    "classifier.save('dnn_model.h5', 'tokenizer.joblib')  # Save the model and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending Logistic Regression Classifier\n",
    "Enhances the TF-IDF Logistic Regression Classifier to include a method for predicting confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfLogisticRegressionClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()  # Initialize TF-IDF vectorizer\n",
    "        self.model = LogisticRegression(max_iter=1000, solver='saga', n_jobs=-1)  # Logistic Regression model\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X_tfidf = self.vectorizer.fit_transform(X)  # Vectorize training data\n",
    "        self.model.fit(X_tfidf, y)  # Fit model\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X_tfidf = self.vectorizer.transform(X)  # Vectorize input data\n",
    "        return self.model.predict(X_tfidf)  # Predict labels\n",
    "    \n",
    "    # Method to get predictions along with confidence scores\n",
    "    def predict_with_confidence(self, X):\n",
    "        X_tfidf = self.vectorizer.transform(X)  # Vectorize input data\n",
    "        probs = self.model.predict_proba(X_tfidf)  # Predict probabilities\n",
    "        predictions = self.model.predict(X_tfidf)  # Predicted labels\n",
    "        confidence_scores = np.max(probs, axis=1)  # Get maximum probability for each prediction\n",
    "        return predictions, confidence_scores\n",
    "        \n",
    "    def save(self, model_path, vectorizer_path):\n",
    "        joblib.dump(self.model, model_path)  # Save model\n",
    "        joblib.dump(self.vectorizer, vectorizer_path)  # Save vectorizer\n",
    "        \n",
    "    def load(self, model_path, vectorizer_path):\n",
    "        self.model = joblib.load(model_path)  # Load model\n",
    "        self.vectorizer = joblib.load(vectorizer_path)  # Load vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Logistic Regression with Confidence Scores\n",
    "Loads the Logistic Regression model and predicts labels and confidence scores for new reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This product is fantastic!\n",
      "Predicted Sentiment: Positive\n",
      "Confidence Score: 0.9993\n",
      "\n",
      "Review: Worst purchase ever.\n",
      "Predicted Sentiment: Negative\n",
      "Confidence Score: 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model and vectorizer\n",
    "classifier = TfidfLogisticRegressionClassifier()  # Initialize the classifier\n",
    "classifier.load('tfidf_logistic_model.joblib', 'tfidf_vectorizer.joblib')  # Load saved model and vectorizer\n",
    "\n",
    "# Example usage\n",
    "new_reviews = [\"This product is fantastic!\", \"Worst purchase ever.\"]  # Test reviews\n",
    "cleaned_new_reviews = [clean_text(review) for review in new_reviews]  # Clean the new reviews\n",
    "predictions, confidence_scores = classifier.predict_with_confidence(cleaned_new_reviews)  # Predict with confidence\n",
    "\n",
    "# Display results\n",
    "for review, label, confidence in zip(new_reviews, predictions, confidence_scores):\n",
    "    sentiment = 'Positive' if label == 1 else 'Negative'  # Map label to sentiment\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Predicted Sentiment: {sentiment}\")\n",
    "    print(f\"Confidence Score: {confidence:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhancing DNN Classifier\n",
    "Extends the DNN classifier to include a method for predicting confidence scores along with class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNClassifier:\n",
    "    def __init__(self, max_words=20000, max_len=300, embedding_dim=128):\n",
    "        self.max_words = max_words  # Vocabulary size\n",
    "        self.max_len = max_len  # Maximum length of sequences\n",
    "        self.embedding_dim = embedding_dim  # Embedding dimensions\n",
    "        self.tokenizer = Tokenizer(num_words=self.max_words)\n",
    "        self.model = None\n",
    "        \n",
    "        # Configure GPU settings\n",
    "        physical_devices = tf.config.list_physical_devices('GPU')\n",
    "        if physical_devices:\n",
    "            try:\n",
    "                # Set memory growth\n",
    "                for gpu in physical_devices:\n",
    "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "                print(len(physical_devices), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "            except RuntimeError as e:\n",
    "                print(e)\n",
    "        else:\n",
    "            print(\"No GPU devices available\")\n",
    "            \n",
    "    def build_model(self):\n",
    "        with tf.device('/GPU:0'):\n",
    "            model = Sequential()\n",
    "            model.add(Embedding(self.max_words, self.embedding_dim, input_length=self.max_len))\n",
    "            model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "            model.add(Dropout(0.5))\n",
    "            model.add(Bidirectional(LSTM(32)))\n",
    "            model.add(Dropout(0.5))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            model.compile(loss='binary_crossentropy',\n",
    "                          optimizer='adam',\n",
    "                          metrics=['accuracy'])\n",
    "        return model\n",
    "        \n",
    "    def fit(self, X, y, epochs=10, batch_size=256):\n",
    "        # Tokenize the text\n",
    "        self.tokenizer.fit_on_texts(X)\n",
    "        sequences = self.tokenizer.texts_to_sequences(X)\n",
    "        X_padded = pad_sequences(sequences, maxlen=self.max_len)\n",
    "        # Build the model\n",
    "        self.model = self.build_model()\n",
    "        # Define early stopping callback\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "        # Fit the model\n",
    "        self.model.fit(X_padded, y, epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks=[early_stop])\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        sequences = self.tokenizer.texts_to_sequences(X)\n",
    "        X_padded = pad_sequences(sequences, maxlen=self.max_len)\n",
    "        predictions = (self.model.predict(X_padded) > 0.5).astype(\"int32\")\n",
    "        return predictions.flatten()\n",
    "    \n",
    "    # Add this method to get predictions with confidence scores\n",
    "    def predict_with_confidence(self, X):\n",
    "        sequences = self.tokenizer.texts_to_sequences(X)\n",
    "        X_padded = pad_sequences(sequences, maxlen=self.max_len)\n",
    "        probs = self.model.predict(X_padded).flatten()\n",
    "        predictions = (probs > 0.5).astype(\"int32\")\n",
    "        confidence_scores = np.where(predictions == 1, probs, 1 - probs)\n",
    "        return predictions, confidence_scores\n",
    "\n",
    "        \n",
    "    def save(self, model_path, tokenizer_path):\n",
    "        self.model.save(model_path)\n",
    "        with open(tokenizer_path, 'wb') as handle:\n",
    "            joblib.dump(self.tokenizer, handle)\n",
    "        \n",
    "    def load(self, model_path, tokenizer_path):\n",
    "        self.model = tf.keras.models.load_model(model_path)\n",
    "        with open(tokenizer_path, 'rb') as handle:\n",
    "            self.tokenizer = joblib.load(handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing DNN Classifier with Confidence Scores\n",
    "Uses the enhanced DNN classifier to predict sentiments and confidence scores for new reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "WARNING:tensorflow:6 out of the last 12505 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7759f1e61550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 403ms/step\n",
      "Review: I love this!\n",
      "Predicted Sentiment: Positive\n",
      "Confidence Score: 0.8054\n",
      "\n",
      "Review: Not what I expected.\n",
      "Predicted Sentiment: Negative\n",
      "Confidence Score: 0.7787\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model and tokenizer\n",
    "classifier = DNNClassifier()  # Initialize the DNN classifier\n",
    "classifier.load('dnn_model.h5', 'tokenizer.joblib')  # Load the saved model and tokenizer\n",
    "\n",
    "# Example usage\n",
    "new_reviews = [\"I love this!\", \"Not what I expected.\"]  # Example reviews\n",
    "cleaned_new_reviews = [clean_text(review) for review in new_reviews]  # Clean the reviews\n",
    "predictions, confidence_scores = classifier.predict_with_confidence(cleaned_new_reviews)  # Predict sentiments\n",
    "\n",
    "# Display results\n",
    "for review, label, confidence in zip(new_reviews, predictions, confidence_scores):\n",
    "    sentiment = 'Positive' if label == 1 else 'Negative'  # Determine sentiment\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Predicted Sentiment: {sentiment}\")\n",
    "    print(f\"Confidence Score: {confidence:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_2_15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
